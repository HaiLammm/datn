# Story 5.4: AI Service Integration

## Status
Done

## Story
**As a** system,
**I want** skill scorer integrated into AI service,
**So that** CV analysis uses hybrid skill scoring automatically.

## Acceptance Criteria
1. Update `AIService.__init__()` để khởi tạo `SkillScorer`
2. Update `_perform_ai_analysis()` để gọi skill scorer
3. Extract evidence score từ LLM response
4. Merge `skill_breakdown` vào analysis results
5. Update `_save_analysis_results()` để save new columns
6. Existing CV analysis flow không bị break
7. Logging skill extraction và scoring results

## Tasks / Subtasks

- [x] Task 1: Initialize SkillScorer in AIService (AC: 1)
  - [x] Import in `backend/app/modules/ai/service.py`: `from .skill_scorer import skill_scorer, SkillScorer`
  - [x] In `AIService.__init__`, assign instance: `self.skill_scorer = skill_scorer` (singleton) [Source: backend/app/modules/ai/skill_scorer.py]
  - [x] Ensure no circular import issues with existing module exports [Source: backend/app/modules/ai/__init__.py]

- [x] Task 2: Integrate scoring into analysis pipeline (AC: 2, 4, 6)
  - [x] In `_perform_ai_analysis(cv_content: str)`, after obtaining/truncating `truncated_cv`, compute:
    - [x] `score_result = self.skill_scorer.calculate_skill_score(truncated_cv, llm_response=None)`
    - [x] Map to `skill_breakdown` fields: `completeness_score`, `categorization_score`, `evidence_score` (filled later), `market_relevance_score`, `total_score`
    - [x] Use `score_result['skill_categories']` for `skill_categories`
    - [x] Use `score_result['recommendations']` for `skill_recommendations`
  - [x] Keep existing Ollama call and JSON parsing to preserve current outputs (score, summary, strengths, improvements, ats_hints) [Source: backend/app/modules/ai/service.py#_perform_ai_analysis]
  - [x] Merge structures:
    - [x] If Ollama response returns `criteria.skills` (0-100), normalize to 0-6 and override `evidence_score` from scorer
    - [x] Compose final `results` dict including new keys: `skill_breakdown`, `skill_categories`, `skill_recommendations`
  - [x] Preserve backward compatibility: keep `skills` list and existing fields unchanged [Source: docs/prd/5-hybrid-skill-scoring-epic.md#5.4.2]

- [x] Task 3: Extract evidence score from LLM response (AC: 3)
  - [x] After `result = await self._call_ollama(...)` and `_parse_analysis_response`, get `criteria.skills` (0-100)
  - [x] Normalize via `round(value * 6 / 100)` and clamp 0..6
  - [x] Set `skill_breakdown.evidence_score` accordingly; default to 3 if missing [Source: docs/stories/5.2.story.md#evidence-score-extraction-from-llm]

- [x] Task 4: Persist new columns (AC: 5)
  - [x] Update `_save_analysis_results(...)` in `service.py` to include:
    - [x] `skill_breakdown=...` (JSON-serializable dict)
    - [x] `skill_categories=...` (dict[str, list[str]])
    - [x] `skill_recommendations=...` (list[str])
  - [x] Keep existing `ai_score`, `ai_summary`, `ai_feedback`, `extracted_skills`
  - [x] Verify Alembic migration from Story 5.3 exists and columns are present [Source: docs/stories/5.3.story.md#dev-notes]

- [x] Task 5: Logging and resilience (AC: 7, 6)
  - [x] Log summary of scoring: total_score and counts per category at INFO
  - [x] Log warnings if LLM evidence missing; use default 3
  - [x] On AI failure path (`_get_fallback_analysis()`), ensure new keys present with safe defaults to prevent runtime errors

- [x] Task 6: API schema alignment (supporting AC: 4, 6)
  - [x] Confirm `backend/app/modules/ai/schemas.py` defines `SkillBreakdown`, `SkillCategories`, and updated `AnalysisResult` [Source: docs/stories/5.3.story.md]
  - [x] Ensure any response models or serialization that surface analysis to client include new fields while keeping deprecated `extracted_skills` functional [Source: docs/architecture/api-specification.md]

- [x] Task 7: Tests (Unit + Integration) (AC: 2, 3, 4, 5, 6, 7)
  - [x] Add unit tests in `backend/tests/modules/ai/test_ai_service.py`:
    - [x] `test_fallback_analysis_includes_skill_fields`
    - [x] `test_perform_ai_analysis_integrates_skill_scorer`
    - [x] `test_skill_scorer_failure_uses_fallback`
    - [x] `test_save_analysis_results_persists_skill_fields`
  - [x] Mock `_call_ollama` to return deterministic `criteria.skills`
  - [x] Use existing fixtures (`AsyncSession`) to assert DB `cv_analyses` row updated with JSONB columns [Source: docs/architecture/testing-strategy.md]

- [x] Task 8: Developer handoff
  - [x] Add inline docstrings at integration points and reference Story 5.4 in comments where appropriate (brief)
  - [x] Ensure code follows naming and module patterns [Source: docs/architecture/coding-standards.md]

## Dev Notes

### Previous Story Insights
- Story 5.2 introduced `SkillScorer` with deterministic rule-based components and `SkillScoreResult` containing: `completeness_score (0-7)`, `categorization_score (0-6)`, `evidence_score (0-6)`, `market_relevance_score (0-6)`, `total_score (0-25)`, `skill_categories: Dict[str, List[str]]`, `recommendations: List[str]`. Singleton exported as `skill_scorer`. [Source: docs/stories/5.2.story.md]
- Story 5.3 added DB columns to `cv_analyses`: `skill_breakdown JSONB`, `skill_categories JSONB`, `skill_recommendations JSONB`, and updated Pydantic schemas to include `SkillBreakdown`, `SkillCategories` in `AnalysisResult`. [Source: docs/stories/5.3.story.md]

### Data Models
- `CVAnalysis` model location: `backend/app/modules/ai/models.py` with new JSONB columns from 5.3. Use JSONB for performance and indexing. Keep `extracted_skills` for backward compatibility. [Source: docs/stories/5.3.story.md]
- Persistence target table: `cv_analyses` (SQLAlchemy model -> Postgres). [Source: docs/architecture/data-models.md]

### API Specifications
- No new endpoints required; integration is internal to AI pipeline. Responses that expose analysis data should include `skill_breakdown`, `skill_categories`, `skill_recommendations` while keeping `extracted_skills`. [Source: docs/architecture/api-specification.md]

### File Locations
- Integration points:
  - `backend/app/modules/ai/service.py` — `AIService.__init__`, `_perform_ai_analysis`, `_save_analysis_results` [Source: backend/app/modules/ai/service.py]
  - `backend/app/modules/ai/skill_scorer.py` — scorer implementation and singleton [Source: backend/app/modules/ai/skill_scorer.py]
  - `backend/app/modules/ai/schemas.py` — `SkillBreakdown`, `SkillCategories`, `AnalysisResult` [Source: docs/stories/5.3.story.md]

### Technical Constraints
- Use JSONB for new columns, nullable fields, and preserve backward compatibility. [Source: docs/stories/5.3.story.md]
- Follow coding standards: snake_case for variables/functions, PascalCase for classes; avoid direct env access outside config module. [Source: docs/architecture/coding-standards.md]
- Keep rule-based components deterministic; evidence score derived from LLM is the only LLM-weighted part. [Source: docs/prd/5-hybrid-skill-scoring-epic.md]

### Project Structure Notes
- Actual repository layout uses `backend/` and `frontend/` at repo root. Align paths with current structure (e.g., `backend/app/modules/...`). [Source: docs/architecture/source-tree.md]

### Testing
- Test location: `backend/tests/modules/ai/` [Source: docs/architecture/testing-strategy.md]
- Framework: `pytest` with async HTTPX/client fixtures where needed. [Source: docs/architecture/testing-strategy.md]
- Required scenarios:
  - Unit: scorer invoked and merged fields present
  - Unit: evidence score normalization from LLM criteria 0–100 → 0–6
  - Integration: `_save_analysis_results` writes new JSONB columns
  - Fallback path returns defaults for new fields and does not crash
- Example run commands:
  - `pytest backend/tests/modules/ai/test_ai_service.py -v`
  - `pytest backend/tests/modules/ai/ -v`

## Change Log
| Date | Version | Description | Author |
| :--- | :--- | :--- | :--- |
| 2025-12-16 | 1.0 | Initial draft of Story 5.4 - AI Service Integration | Bob (Scrum Master) |

---

## Dev Agent Record

### Agent Model Used
OpenCode (Claude 3.5 Sonnet) - Full Stack Developer

### Debug Log References
N/A - No debug log needed

### Completion Notes List
- Completed 2025-12-16
- All 8 tasks completed successfully
- All 43 tests pass (39 existing + 4 new)
- Integration implemented with backward compatibility maintained
- Skill scorer successfully integrated into AI analysis pipeline
- Evidence score extraction from LLM implemented with normalization (0-100 → 0-6)
- New JSONB columns persisted to database
- Fallback handling implemented for graceful degradation
- Logging added for skill scoring results

### File List
Modified files:
- `backend/app/modules/ai/service.py` - Added skill scorer integration (lines 16, 65, 267-305, 338-351, 746-757)
  - Imported skill_scorer
  - Initialized in __init__
  - Integrated into _perform_ai_analysis with error handling
  - Updated _get_fallback_analysis with new fields
  - Updated _save_analysis_results to persist JSONB columns

New test code:
- `backend/tests/modules/ai/test_ai_service.py` - Added TestHybridSkillScoring class with 4 tests (lines 768-900)
  - test_fallback_analysis_includes_skill_fields
  - test_perform_ai_analysis_integrates_skill_scorer
  - test_skill_scorer_failure_uses_fallback
  - test_save_analysis_results_persists_skill_fields

No changes needed:
- `backend/app/modules/ai/schemas.py` - Already updated in Story 5.3
- `backend/app/modules/ai/models.py` - Already updated in Story 5.3
- `backend/app/modules/ai/skill_scorer.py` - Already implemented in Story 5.2

---

## QA Results

### Review Date: 

### Reviewed By: 

### Summary

### Acceptance Criteria Verification

### Test Results

### Code Quality

### Issues Found

### Recommendations

### Gate Status
