# Story 5.3: Database Schema & API Response Update

## Status
Ready for Review

## Story
**As a** developer,
**I want** updated database schema and API response,
**So that** skill breakdown data is persisted and returned to clients.

## Acceptance Criteria
1. Tao Alembic migration them columns: skill_breakdown, skill_categories, skill_recommendations
2. Columns su dung JSONB type cho flexibility
3. Update `CVAnalysis` model voi new columns
4. Update `AnalysisResult` schema voi SkillBreakdown, SkillCategories
5. Backward compatibility: existing `extracted_skills` field van hoat dong
6. Migration co the rollback safely

## Tasks / Subtasks

- [x] **Task 1: Create Alembic Migration for New Columns (AC: 1, 2, 6)**
  - [x] Generate new migration file using `alembic revision --autogenerate -m "add_skill_breakdown_columns"`
  - [x] Add `skill_breakdown` column with JSONB type (nullable)
  - [x] Add `skill_categories` column with JSONB type (nullable)
  - [x] Add `skill_recommendations` column with JSONB type (nullable - use JSONB instead of TEXT[] for consistency)
  - [x] Implement `downgrade()` function to drop all 3 columns safely
  - [x] Verify migration can be applied and rolled back without data loss

- [x] **Task 2: Update CVAnalysis Model (AC: 3, 5)**
  - [x] Open `backend/app/modules/ai/models.py`
  - [x] Import `JSONB` from `sqlalchemy.dialects.postgresql` if not already imported
  - [x] Add `skill_breakdown: Mapped[dict | None]` column with JSONB type
  - [x] Add `skill_categories: Mapped[dict | None]` column with JSONB type
  - [x] Add `skill_recommendations: Mapped[list[str] | None]` column with JSONB type
  - [x] Keep existing `extracted_skills` column unchanged for backward compatibility
  - [x] Add type hints and docstrings

- [x] **Task 3: Create New Pydantic Schemas (AC: 4)**
  - [x] Open `backend/app/modules/ai/schemas.py`
  - [x] Create `SkillBreakdown` schema with fields:
    - `completeness_score: int` (0-7)
    - `categorization_score: int` (0-6)
    - `evidence_score: int` (0-6)
    - `market_relevance_score: int` (0-6)
    - `total_score: int` (0-25)
  - [x] Create `SkillCategories` schema with fields:
    - `programming_languages: List[str]`
    - `frameworks: List[str]`
    - `databases: List[str]`
    - `devops: List[str]`
    - `soft_skills: List[str]`
    - `ai_ml: List[str]` (to match existing taxonomy)
  - [x] Add validators to ensure scores are within valid ranges

- [x] **Task 4: Update AnalysisResult Schema (AC: 4, 5)**
  - [x] Add `skill_breakdown: Optional[SkillBreakdown] = None` field
  - [x] Add `skill_categories: Optional[SkillCategories] = None` field
  - [x] Add `skill_recommendations: Optional[List[str]] = None` field
  - [x] Add computed_field to convert DB JSONB to typed SkillBreakdown
  - [x] Add computed_field to convert DB JSONB to typed SkillCategories
  - [x] Keep existing `extracted_skills` field for backward compatibility
  - [x] Add deprecation notice in docstring for `extracted_skills`

- [x] **Task 5: Run and Verify Migration (AC: 1, 6)**
  - [x] Run `alembic upgrade head` to apply migration
  - [x] Verify columns exist in database: `\d cv_analyses` in psql
  - [x] Run `alembic downgrade -1` to test rollback
  - [x] Run `alembic upgrade head` again to confirm re-application

- [x] **Task 6: Write Unit Tests for New Schemas (AC: 4)**
  - [x] Create test file `backend/tests/modules/ai/test_schemas.py` if not exists
  - [x] Test `SkillBreakdown` schema:
    - `test_skill_breakdown_valid_scores`
    - `test_skill_breakdown_total_matches_sum`
    - `test_skill_breakdown_from_dict`
  - [x] Test `SkillCategories` schema:
    - `test_skill_categories_valid`
    - `test_skill_categories_empty_lists`
    - `test_skill_categories_from_dict`
  - [x] Test `AnalysisResult` with new fields:
    - `test_analysis_result_with_skill_breakdown`
    - `test_analysis_result_backward_compatible`
  - [x] Run tests: `pytest backend/tests/modules/ai/test_schemas.py -v`

- [x] **Task 7: Write Integration Test for DB Operations (AC: 3, 5)**
  - [x] Create or update test file for model operations
  - [x] Test creating CVAnalysis with new columns
  - [x] Test reading CVAnalysis with new columns
  - [x] Test that existing records without new columns still work
  - [x] Verify JSONB queries work correctly

- [x] **Task 8: Update Module Exports**
  - [x] Update `backend/app/modules/ai/__init__.py` to export:
    - `SkillBreakdown`
    - `SkillCategories`
  - [x] Verify imports work correctly from module

## Dev Notes

### Previous Story Insights
From Story 5.2 (Hybrid Skill Scorer Implementation):
- `SkillScorer` class created at `backend/app/modules/ai/skill_scorer.py`
- `SkillScoreResult` TypedDict already defined with fields:
  - `completeness_score: int` (0-7)
  - `categorization_score: int` (0-6)
  - `evidence_score: int` (0-6)
  - `market_relevance_score: int` (0-6)
  - `total_score: int` (0-25)
  - `skill_categories: Dict[str, List[str]]`
  - `recommendations: List[str]`
- The Pydantic schemas should mirror `SkillScoreResult` structure
- 6 categories in taxonomy: programming_languages, frameworks, databases, devops, soft_skills, ai_ml

[Source: docs/stories/5.2.story.md#dev-agent-record]

### Data Models
- **CVAnalysis Model Location:** `backend/app/modules/ai/models.py`
- **Current columns:**
  - `id` (UUID, primary key)
  - `cv_id` (UUID, foreign key to cvs.id)
  - `status` (String, enum: PENDING, PROCESSING, COMPLETED, FAILED)
  - `ai_score` (Integer, nullable)
  - `ai_summary` (Text, nullable)
  - `ai_feedback` (JSON, nullable)
  - `extracted_skills` (JSON, nullable - list of strings)
  - `created_at` (DateTime)
  - `updated_at` (DateTime)
- **New columns to add:**
  - `skill_breakdown` (JSONB, nullable) - stores SkillBreakdown data
  - `skill_categories` (JSONB, nullable) - stores categorized skills dict
  - `skill_recommendations` (JSONB, nullable) - stores list of recommendations
- **Important:** Use JSONB (not JSON) for PostgreSQL for better query performance and indexing

[Source: backend/app/modules/ai/models.py]

### API Specifications
- **AnalysisResult Schema Location:** `backend/app/modules/ai/schemas.py`
- **Current fields:** id, cv_id, status, ai_score, ai_summary, ai_feedback, extracted_skills, created_at, updated_at
- **Computed fields:** experience_breakdown, formatting_feedback, ats_hints, strengths, improvements, criteria_explanation
- **New fields to add:**
  - `skill_breakdown: Optional[SkillBreakdown]`
  - `skill_categories: Optional[SkillCategories]`
  - `skill_recommendations: Optional[List[str]]`
- **Backward compatibility:** Keep `extracted_skills` as deprecated field

[Source: backend/app/modules/ai/schemas.py]

### File Locations
- **Files to Modify:**
  - `backend/app/modules/ai/models.py` - Add new columns to CVAnalysis
  - `backend/app/modules/ai/schemas.py` - Add new Pydantic schemas
  - `backend/app/modules/ai/__init__.py` - Add new exports
- **Files to Create:**
  - `backend/alembic/versions/xxx_add_skill_breakdown_columns.py` - Migration file
  - `backend/tests/modules/ai/test_schemas.py` - Schema unit tests (if not exists)
- **Files to Reference:**
  - `backend/app/modules/ai/skill_scorer.py` - SkillScoreResult TypedDict structure
  - `backend/alembic/versions/e501602af441_create_cv_analyses_table.py` - Migration example

[Source: docs/architecture/source-tree.md, docs/prd/5-hybrid-skill-scoring-epic.md]

### Technical Constraints
- Use `JSONB` type (not `JSON`) for PostgreSQL - better performance and supports indexing
- Import JSONB: `from sqlalchemy.dialects.postgresql import JSONB`
- Keep existing `extracted_skills` column - do NOT remove or rename
- All new columns must be nullable to support existing records
- Migration must have both `upgrade()` and `downgrade()` functions
- Pydantic schemas use `from_attributes = True` for ORM compatibility
- Use `Optional` type hints for nullable fields
- Follow snake_case for variables/columns, PascalCase for classes

[Source: docs/architecture/coding-standards.md, docs/architecture/tech-stack.md]

### Migration Template
```python
"""add skill breakdown columns

Revision ID: <auto-generated>
Revises: e501602af441
Create Date: <auto-generated>
"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import JSONB


revision: str = '<auto-generated>'
down_revision: Union[str, Sequence[str], None] = 'e501602af441'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Add skill breakdown columns to cv_analyses table."""
    op.add_column('cv_analyses', sa.Column('skill_breakdown', JSONB(), nullable=True))
    op.add_column('cv_analyses', sa.Column('skill_categories', JSONB(), nullable=True))
    op.add_column('cv_analyses', sa.Column('skill_recommendations', JSONB(), nullable=True))


def downgrade() -> None:
    """Remove skill breakdown columns from cv_analyses table."""
    op.drop_column('cv_analyses', 'skill_recommendations')
    op.drop_column('cv_analyses', 'skill_categories')
    op.drop_column('cv_analyses', 'skill_breakdown')
```

[Source: backend/alembic/versions/e501602af441_create_cv_analyses_table.py]

### Schema Examples
```python
# New schemas to add to schemas.py

class SkillBreakdown(BaseModel):
    """Hybrid skill scoring breakdown."""
    completeness_score: int = Field(ge=0, le=7, description="Skill completeness (0-7)")
    categorization_score: int = Field(ge=0, le=6, description="Category coverage (0-6)")
    evidence_score: int = Field(ge=0, le=6, description="Evidence from LLM (0-6)")
    market_relevance_score: int = Field(ge=0, le=6, description="Hot skills match (0-6)")
    total_score: int = Field(ge=0, le=25, description="Total skill score (0-25)")


class SkillCategories(BaseModel):
    """Categorized skills extracted from CV."""
    programming_languages: List[str] = []
    frameworks: List[str] = []
    databases: List[str] = []
    devops: List[str] = []
    soft_skills: List[str] = []
    ai_ml: List[str] = []
```

[Source: docs/prd/5-hybrid-skill-scoring-epic.md#5.4.2]

## Testing

### Test File Location
- `backend/tests/modules/ai/test_schemas.py`

### Test Framework
- `pytest` with standard fixtures
- Use `conftest.py` fixtures from `backend/tests/`

### Test Patterns
```python
import pytest
from pydantic import ValidationError
from backend.app.modules.ai.schemas import (
    SkillBreakdown,
    SkillCategories,
    AnalysisResult,
)


class TestSkillBreakdown:
    def test_valid_scores(self):
        breakdown = SkillBreakdown(
            completeness_score=5,
            categorization_score=4,
            evidence_score=3,
            market_relevance_score=4,
            total_score=16,
        )
        assert breakdown.total_score == 16

    def test_invalid_completeness_score_too_high(self):
        with pytest.raises(ValidationError):
            SkillBreakdown(
                completeness_score=10,  # max is 7
                categorization_score=4,
                evidence_score=3,
                market_relevance_score=4,
                total_score=21,
            )

    def test_from_dict(self):
        data = {
            "completeness_score": 7,
            "categorization_score": 6,
            "evidence_score": 6,
            "market_relevance_score": 6,
            "total_score": 25,
        }
        breakdown = SkillBreakdown(**data)
        assert breakdown.completeness_score == 7


class TestSkillCategories:
    def test_valid_categories(self):
        cats = SkillCategories(
            programming_languages=["python", "javascript"],
            frameworks=["react", "fastapi"],
            databases=["postgresql"],
            devops=["docker"],
            soft_skills=["teamwork"],
            ai_ml=["machine learning"],
        )
        assert len(cats.programming_languages) == 2

    def test_empty_categories(self):
        cats = SkillCategories()
        assert cats.programming_languages == []
        assert cats.frameworks == []


class TestAnalysisResultBackwardCompatibility:
    def test_with_extracted_skills_only(self):
        """Old records without new fields should still work."""
        result = AnalysisResult(
            id="...",
            cv_id="...",
            status="COMPLETED",
            extracted_skills=["python", "react"],
            # No skill_breakdown, skill_categories, skill_recommendations
        )
        assert result.extracted_skills == ["python", "react"]
        assert result.skill_breakdown is None
```

### Running Tests
```bash
# Run schema tests
pytest backend/tests/modules/ai/test_schemas.py -v

# Run all AI module tests
pytest backend/tests/modules/ai/ -v

# Run with coverage
pytest backend/tests/modules/ai/test_schemas.py -v --cov=backend/app/modules/ai/schemas
```

[Source: docs/architecture/testing-strategy.md]

## Change Log
| Date | Version | Description | Author |
| :--- | :--- | :--- | :--- |
| 2024-12-16 | 1.0 | Initial draft of Story 5.3 - Database Schema & API Response Update | Bob (Scrum Master) |
| 2024-12-16 | 1.1 | Implementation complete - All 8 tasks done, 32 tests added, migration verified | James (Dev Agent) |

---

## Dev Agent Record

### Agent Model Used
Claude Sonnet 4 (via OpenCode)

### Debug Log References
No debug issues encountered during implementation.

### Completion Notes List
- All 8 tasks completed successfully
- Alembic migration tested with upgrade, downgrade, and re-upgrade cycles
- All 273 AI module tests pass (including 32 new tests for this story)
- Pre-existing mypy errors with `@computed_field` decorator pattern remain (known Pydantic v2 + mypy compatibility issue)
- Pre-existing deprecation warnings for `class Config` pattern in Pydantic schemas remain (not in scope)
- Backward compatibility verified: existing `extracted_skills` field preserved, new fields are nullable

### File List
**Created:**
- `backend/alembic/versions/f7a3b2c1d4e5_add_skill_breakdown_columns.py` - Alembic migration for new JSONB columns
- `backend/tests/modules/ai/test_schemas.py` - Unit tests for SkillBreakdown, SkillCategories, AnalysisResult schemas (24 tests)
- `backend/tests/modules/ai/test_cv_analysis_model.py` - Integration tests for CVAnalysis model with new columns (8 tests)

**Modified:**
- `backend/app/modules/ai/models.py` - Added skill_breakdown, skill_categories, skill_recommendations columns with JSONB type
- `backend/app/modules/ai/schemas.py` - Added SkillBreakdown, SkillCategories schemas; updated AnalysisResult with new fields
- `backend/app/modules/ai/__init__.py` - Exported SkillBreakdown, SkillCategories schemas

---

## QA Results

### Review Date: 2024-12-16

### Reviewed By: Quinn (Test Architect)

### Risk Assessment
**Review Depth: Standard** - Low risk story (database schema update, no auth/security changes, <500 lines diff)

### Code Quality Assessment

**Overall: EXCELLENT** - Implementation is clean, well-documented, and follows established patterns.

**Strengths:**
- Migration file follows correct pattern with proper `upgrade()` and `downgrade()` functions
- JSONB type used correctly for PostgreSQL performance (not JSON)
- All new columns are nullable, ensuring backward compatibility
- Pydantic schemas include proper `Field` validators with `ge`/`le` constraints
- Type hints are comprehensive throughout
- Module exports updated correctly in `__init__.py`
- Docstrings explain purpose clearly (including deprecation notice for `extracted_skills`)

**Code Quality Metrics:**
- Type safety: Mapped types in SQLAlchemy model match Pydantic schemas
- Consistency: New columns mirror `SkillScoreResult` TypedDict from Story 5.2
- Documentation: SQL comments added to columns, Python docstrings present

### Requirements Traceability

| AC# | Requirement | Test Coverage | Status |
|-----|-------------|---------------|--------|
| AC1 | Alembic migration with 3 columns | `f7a3b2c1d4e5_add_skill_breakdown_columns.py` created | PASS |
| AC2 | JSONB type for flexibility | Lines 30-32 use `JSONB()` type | PASS |
| AC3 | CVAnalysis model updated | `models.py:37-52` adds columns with docstrings | PASS |
| AC4 | AnalysisResult schema with new types | `schemas.py:21-51` + `schemas.py:70-72` | PASS |
| AC5 | Backward compatibility | `test_analysis_result_backward_compatible`, `test_cv_analysis_without_new_columns_backward_compatible` | PASS |
| AC6 | Migration rollback safe | `downgrade()` drops all 3 columns | PASS |

### Test Architecture Assessment

**Test Coverage: 32 tests (24 schema + 8 model)**

| Test Category | Count | Coverage |
|---------------|-------|----------|
| SkillBreakdown validation | 11 | Boundary values, invalid inputs |
| SkillCategories validation | 5 | Empty, partial, dict conversion |
| AnalysisResult integration | 8 | New fields, backward compat, dict coercion |
| CVAnalysis model | 8 | JSONB operations, query patterns |

**Test Patterns:**
- Given-When-Then structure followed implicitly
- Edge cases covered: min/max scores, empty lists, missing keys
- Dict-to-object coercion tested (simulating DB → Pydantic flow)

### Compliance Check

- Coding Standards: [PASS] snake_case columns, PascalCase classes
- Project Structure: [PASS] Files in correct locations per architecture
- Testing Strategy: [PASS] Unit tests follow pyramid (fast, isolated)
- All ACs Met: [PASS] All 6 acceptance criteria validated

### Refactoring Performed

No refactoring performed - implementation is clean.

### Improvements Checklist

[x] All 32 tests passing
[x] Module exports working correctly
[x] Backward compatibility verified
[x] Migration rollback verified in Dev Notes

**Deferred (pre-existing, not in scope):**
- [ ] `class Config` deprecation warnings in Pydantic schemas (project-wide issue)
- [ ] `datetime.utcnow()` deprecation in test files (use `datetime.now(datetime.UTC)`)

### Security Review

**Status: PASS - No security concerns**
- No auth changes
- No PII exposure
- JSONB fields are nullable (no data leak risk)

### Performance Considerations

**Status: PASS**
- JSONB type enables PostgreSQL indexing if needed later
- Nullable columns avoid storage overhead for existing records
- No N+1 query risks introduced

### Files Modified During Review

None - no modifications made by QA.

### Gate Status

**Gate: PASS** → `docs/qa/gates/5.3-database-schema-api-response-update.yml`

**Quality Score: 100/100**
- No FAILs
- No CONCERNS
- All acceptance criteria met

### Recommended Status

[PASS] Ready for Done

Story implementation is complete and all acceptance criteria are validated. This unblocks Story 5.4 (AI Service Integration) which will integrate the SkillScorer into the live CV analysis flow.
