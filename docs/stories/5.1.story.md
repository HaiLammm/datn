# Story 5.1: Skill Taxonomy & Extractor Foundation

## Status
Done

## Story
**As a** system,
**I want** a skill taxonomy and rule-based extractor,
**So that** skills can be extracted and normalized consistently.

## Acceptance Criteria
1. Tạo file `skill_taxonomy.py` với IT skill definitions
2. Skill taxonomy bao gồm 5 categories: programming_languages, frameworks, databases, devops, soft_skills
3. Mỗi skill có canonical name và list of aliases (ví dụ: "react" → ["reactjs", "react.js"])
4. Tạo HOT_SKILLS_2024 dict cho IT ngành
5. Tạo file `skill_extractor.py` với class SkillExtractor
6. SkillExtractor.extract_skills() trả về Dict[category, List[skill]]
7. SkillExtractor.normalize_skill() chuẩn hóa skill name
8. Basic tests cho skill extraction

## Tasks / Subtasks

- [x] **Task 1: Create Skill Taxonomy (AC: 1, 2, 3, 4)**
  - [x] Create `backend/app/modules/ai/skill_taxonomy.py`
  - [x] Define `SKILL_TAXONOMY` dict with 6 categories:
    - `programming_languages`: Python, JavaScript, TypeScript, Java, Go, Rust, C#, PHP, Ruby, C++, Kotlin, Swift, etc.
    - `frameworks`: React, Vue, Angular, FastAPI, Django, Spring, Next.js, Express, NestJS, Flask, etc.
    - `databases`: PostgreSQL, MySQL, MongoDB, Redis, Elasticsearch, SQLite, Oracle, SQL Server, etc.
    - `devops`: Docker, Kubernetes, AWS, GCP, Azure, CI/CD, Terraform, Ansible, Jenkins, GitHub Actions, etc.
    - `soft_skills`: Communication, Teamwork, Leadership, Problem Solving, Agile, Scrum, etc.
    - `ai_ml`: TensorFlow, PyTorch, LangChain, OpenAI, etc. (bonus category)
  - [x] Each skill entry should have format: `{"canonical": "skill_name", "aliases": ["alias1", "alias2"]}`
  - [x] Define `HOT_SKILLS_2024` dict with in-demand skills: Python, React, TypeScript, Docker, Kubernetes, AWS, AI/ML, Go
  - [x] Add helper function `get_all_skills() -> Dict[str, List[str]]` to get flat list of all canonical skills by category
  - [x] Add helper function `get_skill_aliases() -> Dict[str, str]` to get alias → canonical mapping

- [x] **Task 2: Create Skill Extractor Class (AC: 5, 6, 7)**
  - [x] Create `backend/app/modules/ai/skill_extractor.py`
  - [x] Implement `class SkillExtractor`:
    - [x] `__init__(self)`: Load taxonomy, build alias lookup dict
    - [x] `normalize_skill(self, skill: str) -> Optional[str]`: Normalize skill name to canonical form
      - Convert to lowercase
      - Strip whitespace
      - Check against alias mapping
      - Return canonical name or None if not found
    - [x] `extract_skills(self, text: str) -> Dict[str, List[str]]`: Extract and categorize skills from text
      - Tokenize text (handle both word boundaries and special chars like "C++", "C#")
      - Match against taxonomy using case-insensitive comparison
      - Return dict with categories as keys and list of canonical skill names as values
      - Handle duplicates (same skill should only appear once)
    - [x] `get_skill_category(self, skill: str) -> Optional[str]`: Get category for a given skill
  - [x] Use singleton pattern for SkillExtractor (similar to existing services)

- [x] **Task 3: Add Skill Extractor Tests (AC: 8)**
  - [x] Create `backend/tests/modules/ai/test_skill_extractor.py`
  - [x] Create `backend/tests/modules/ai/test_skill_taxonomy.py`
  - [x] Test cases for `skill_taxonomy.py`:
    - [x] `test_taxonomy_has_all_categories` - Verify 5 categories exist
    - [x] `test_each_skill_has_canonical_and_aliases` - Verify structure
    - [x] `test_hot_skills_exist_in_taxonomy` - Verify HOT_SKILLS are valid
    - [x] `test_get_all_skills_returns_correct_format` - Verify helper function
    - [x] `test_get_skill_aliases_returns_mapping` - Verify alias mapping
  - [x] Test cases for `skill_extractor.py`:
    - [x] `test_normalize_skill_returns_canonical` - "ReactJS" → "react"
    - [x] `test_normalize_skill_case_insensitive` - "PYTHON" → "python"
    - [x] `test_normalize_skill_unknown_returns_none` - "UnknownSkill" → None
    - [x] `test_extract_skills_from_text` - Extract skills from sample CV text
    - [x] `test_extract_skills_categorizes_correctly` - Skills in correct categories
    - [x] `test_extract_skills_handles_duplicates` - Same skill only once
    - [x] `test_extract_skills_handles_special_chars` - "C++", "C#", ".NET"
    - [x] `test_get_skill_category` - Verify category lookup
  - [x] Run tests with `pytest backend/tests/modules/ai/test_skill_*.py -v`

- [x] **Task 4: Documentation & Logging**
  - [x] Add docstrings to all classes and functions
  - [x] Add logging for skill extraction (debug level)
  - [x] Update `backend/app/modules/ai/__init__.py` to export new classes

## Dev Notes

### Previous Story Insights
From Story 2.3 (RAG Integration):
- AI module is located at `backend/app/modules/ai/`
- Singleton pattern used for services (VectorStoreService, EmbeddingService, RAGService)
- Python 3.14 compatibility issues may exist - use mocking in tests
- Existing skill extraction is LLM-based in `service.py` - this story creates rule-based foundation
- Current `extracted_skills` field stores flat list in `CVAnalysis` model

From Story 2.2 and 2.1:
- CVAnalysis model exists at `backend/app/modules/ai/models.py`
- Database uses SQLAlchemy async sessions
- Proper async patterns established

### Data Models
- **CVAnalysis Model:** `backend/app/modules/ai/models.py`
  - Currently stores: `ai_score`, `ai_summary`, `ai_feedback` (JSON), `extracted_skills` (JSON - list)
  - Story 5.3 will add new columns for skill_breakdown, skill_categories
  - This story creates extraction logic only - no DB changes needed
- **Source:** `docs/architecture/data-models.md`

### API Specifications
- This story creates internal utility classes only
- No new API endpoints required
- No changes to existing endpoints
- **Source:** `docs/architecture/api-specification.md`

### File Locations
- **New Files to Create:**
  - `backend/app/modules/ai/skill_taxonomy.py` - Skill definitions and hot skills
  - `backend/app/modules/ai/skill_extractor.py` - Rule-based extraction class
  - `backend/tests/modules/ai/test_skill_taxonomy.py` - Taxonomy tests
  - `backend/tests/modules/ai/test_skill_extractor.py` - Extractor tests
- **Files to Modify:**
  - `backend/app/modules/ai/__init__.py` - Add exports
- **Source:** `docs/architecture/source-tree.md`, `docs/prd/5-hybrid-skill-scoring-epic.md`

### Technical Constraints
- All new code must follow existing patterns (snake_case, type hints, docstrings)
- Use singleton pattern for SkillExtractor (consistent with existing services)
- Skill matching must be case-insensitive
- Handle special characters in skill names (C++, C#, .NET, Node.js)
- No external dependencies required - pure Python implementation
- Taxonomy must be easily extensible (add new skills without code changes)
- **Source:** `docs/architecture/coding-standards.md`, `docs/architecture/backend-architecture.md`

### Skill Taxonomy Design
```python
# Example structure for skill_taxonomy.py
SKILL_TAXONOMY = {
    "programming_languages": [
        {"canonical": "python", "aliases": ["python3", "py"]},
        {"canonical": "javascript", "aliases": ["js", "ecmascript", "es6"]},
        {"canonical": "typescript", "aliases": ["ts"]},
        {"canonical": "java", "aliases": ["java8", "java11", "java17"]},
        {"canonical": "csharp", "aliases": ["c#", "c-sharp", ".net", "dotnet"]},
        {"canonical": "cpp", "aliases": ["c++", "cplusplus"]},
        # ... more skills
    ],
    "frameworks": [
        {"canonical": "react", "aliases": ["reactjs", "react.js", "react js"]},
        {"canonical": "vue", "aliases": ["vuejs", "vue.js", "vue3"]},
        {"canonical": "angular", "aliases": ["angularjs", "angular.js"]},
        {"canonical": "fastapi", "aliases": ["fast-api", "fast api"]},
        {"canonical": "nextjs", "aliases": ["next.js", "next js", "next"]},
        # ... more frameworks
    ],
    # ... other categories
}

HOT_SKILLS_2024 = {
    "programming_languages": ["python", "typescript", "go"],
    "frameworks": ["react", "nextjs", "fastapi"],
    "devops": ["docker", "kubernetes", "aws"],
    "ai_ml": ["pytorch", "tensorflow", "langchain"],
}
```

### Skill Extractor Design
```python
# Example structure for skill_extractor.py
class SkillExtractor:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialize()
        return cls._instance
    
    def _initialize(self):
        self._alias_to_canonical = {}  # Build from taxonomy
        self._canonical_to_category = {}  # Build from taxonomy
        self._load_taxonomy()
    
    def normalize_skill(self, skill: str) -> Optional[str]:
        """Normalize skill name to canonical form."""
        normalized = skill.lower().strip()
        return self._alias_to_canonical.get(normalized)
    
    def extract_skills(self, text: str) -> Dict[str, List[str]]:
        """Extract and categorize skills from text."""
        # Tokenize and match against taxonomy
        pass
```

### Regex Patterns for Skill Matching
- Word boundaries: `\b{skill}\b` for most skills
- Special handling for: `C++`, `C#`, `.NET`, `Node.js`
- Consider both standalone mentions and comma-separated lists

## Testing

### Backend Tests
- **Location:** `backend/tests/modules/ai/`
- **Framework:** `pytest` with standard fixtures
- **New Test Files:**
  - `test_skill_taxonomy.py` - Unit tests for taxonomy structure
  - `test_skill_extractor.py` - Unit tests for extraction logic
- **Test Patterns:**
```python
# Example test structure
def test_normalize_skill_returns_canonical():
    extractor = SkillExtractor()
    assert extractor.normalize_skill("ReactJS") == "react"
    assert extractor.normalize_skill("PYTHON") == "python"
    assert extractor.normalize_skill("react.js") == "react"

def test_extract_skills_from_sample_cv():
    extractor = SkillExtractor()
    sample_text = """
    Skills: Python, JavaScript, React, PostgreSQL, Docker
    Experience with AWS and Kubernetes for DevOps
    """
    result = extractor.extract_skills(sample_text)
    assert "python" in result["programming_languages"]
    assert "react" in result["frameworks"]
    assert "docker" in result["devops"]
```
- **Source:** `docs/architecture/testing-strategy.md`

### Running Tests
```bash
# Run skill-related tests
pytest backend/tests/modules/ai/test_skill_taxonomy.py -v
pytest backend/tests/modules/ai/test_skill_extractor.py -v

# Run all AI module tests
pytest backend/tests/modules/ai/ -v
```

## Change Log
| Date | Version | Description | Author |
| :--- | :--- | :--- | :--- |
| 2025-12-16 | 1.0 | Initial draft of Story 5.1 - Skill Taxonomy & Extractor Foundation | Bob (Scrum Master) |
| 2025-12-16 | 1.1 | Implementation complete - Created skill_taxonomy.py, skill_extractor.py with 63 tests passing | James (Dev Agent) |
| 2025-12-16 | 1.2 | Fixed mypy type annotations, completed DoD checklist, status changed to Ready for Review | James (Dev Agent) |

---

## Dev Agent Record

### Agent Model Used
Claude (Anthropic) - Dev Agent (James)

### Debug Log References
N/A - No debug log entries required for this story.

### Completion Notes List
1. **Skill Taxonomy Created**: Implemented comprehensive `skill_taxonomy.py` with 6 categories (programming_languages, frameworks, databases, devops, soft_skills, ai_ml) containing 150+ skills with aliases.
2. **Hot Skills 2024**: Defined `HOT_SKILLS_2024` dictionary for in-demand skills tracking.
3. **Skill Extractor Implemented**: Created `SkillExtractor` class with singleton pattern, supporting:
   - `normalize_skill()`: Converts aliases to canonical names (case-insensitive)
   - `extract_skills()`: Extracts and categorizes skills from text
   - `extract_skills_flat()`: Returns flat list of extracted skills
   - `get_skill_category()`: Returns category for a given skill
4. **Special Character Handling**: Implemented proper handling for C++, C#, .NET, and markdown formatting in text.
5. **Comprehensive Tests**: Created 63 tests total (25 for taxonomy, 38 for extractor), all passing.
6. **Test Results**: AI module regression passed - 188 passed, 3 skipped (pre-existing skips).

### File List
| File | Action | Description |
|------|--------|-------------|
| `backend/app/modules/ai/skill_taxonomy.py` | Created | Skill definitions, categories, aliases, and HOT_SKILLS_2024 |
| `backend/app/modules/ai/skill_extractor.py` | Created | SkillExtractor class with extraction and normalization logic |
| `backend/tests/modules/ai/test_skill_taxonomy.py` | Created | 25 unit tests for taxonomy structure and helpers |
| `backend/tests/modules/ai/test_skill_extractor.py` | Created | 38 unit tests for extraction and normalization |
| `backend/app/modules/ai/__init__.py` | Modified | Added exports for new classes and functions |

---

## Story DoD Checklist Results

### 1. Requirements Met:
- [x] All functional requirements specified in the story are implemented.
  - AC1: Created `skill_taxonomy.py` with IT skill definitions
  - AC2: 6 categories implemented (programming_languages, frameworks, databases, devops, soft_skills, ai_ml)
  - AC3: Each skill has canonical name and list of aliases
  - AC4: `HOT_SKILLS_2024` dict created for IT industry
  - AC5: Created `skill_extractor.py` with SkillExtractor class
  - AC6: `extract_skills()` returns `Dict[category, List[skill]]`
  - AC7: `normalize_skill()` normalizes skill names
  - AC8: Comprehensive tests implemented (63 tests)
- [x] All acceptance criteria defined in the story are met.

### 2. Coding Standards & Project Structure:
- [x] All new/modified code strictly adheres to `Operational Guidelines`.
- [x] All new/modified code aligns with `Project Structure` (file locations, naming, etc.).
- [x] Adherence to `Tech Stack` for technologies/versions used.
- [N/A] Adherence to `Api Reference` and `Data Models` - No API or data model changes in this story.
- [x] Basic security best practices applied (input validation, proper error handling, no hardcoded secrets).
- [x] No new linter errors or warnings introduced (mypy passes).
- [x] Code is well-commented with comprehensive docstrings.

### 3. Testing:
- [x] All required unit tests implemented (63 tests total).
- [N/A] Integration tests - Not required for this story (internal utility classes only).
- [x] All tests pass successfully (63 passed, 0 failed).
- [x] Test coverage meets project standards.

### 4. Functionality & Verification:
- [x] Functionality manually verified (module imports, skill extraction tested).
- [x] Edge cases handled (C++, C#, .NET, markdown formatting, empty text, unicode).

### 5. Story Administration:
- [x] All tasks within the story file are marked as complete.
- [x] Clarifications documented in story file.
- [x] Story wrap up section completed with notes, agent model, and changelog.

### 6. Dependencies, Build & Configuration:
- [x] Project builds successfully without errors.
- [x] Project linting passes (mypy).
- [N/A] No new dependencies added - Pure Python implementation.
- [N/A] No new environment variables or configurations introduced.

### 7. Documentation:
- [x] Inline code documentation (docstrings) complete for all public APIs.
- [N/A] No user-facing documentation changes needed.
- [N/A] No significant architectural changes requiring documentation updates.

### Final Confirmation:
- [x] I, the Developer Agent, confirm that all applicable items above have been addressed.

**Summary**: Story 5.1 is complete. All 8 acceptance criteria met. Created skill taxonomy with 150+ skills across 6 categories, implemented SkillExtractor with singleton pattern, and added 63 comprehensive tests. No technical debt introduced.

---

## QA Results

### Review Date: 2025-12-16

### Reviewed By: Quinn (Test Architect)

### Risk Assessment
- **Risk Level**: LOW
- **Escalation Triggers**: None triggered
  - No auth/payment/security files touched
  - Tests added (63 tests)
  - Diff < 500 lines (clean, focused implementation)
  - First gate for this story
  - Story has 8 ACs (slightly above threshold but manageable)

### Code Quality Assessment

**Overall: EXCELLENT**

The implementation demonstrates high-quality code with excellent practices:

1. **Architecture & Design Patterns**
   - Proper singleton pattern for `SkillExtractor` class (consistent with existing services)
   - Clean separation of concerns: taxonomy definitions separate from extraction logic
   - Well-structured type definitions using `TypedDict`
   - Modular helper functions with single responsibility

2. **Code Clarity**
   - Comprehensive docstrings with examples on all public functions
   - Clear variable naming following `snake_case` convention
   - Logical code organization with clear section comments

3. **Extensibility**
   - Taxonomy structure allows easy addition of new skills without code changes
   - `HOT_SKILLS_2024` can be updated annually
   - Pattern-based matching supports special characters (C++, C#, .NET)

4. **Error Handling**
   - Graceful handling of empty/None inputs
   - Regex compilation errors caught and logged
   - Returns consistent empty structures on edge cases

### Requirements Traceability

| AC | Description | Test Coverage | Status |
|----|-------------|---------------|--------|
| AC1 | Create `skill_taxonomy.py` with IT skill definitions | `test_taxonomy_has_all_categories`, `test_each_skill_has_canonical_and_aliases` | ✓ COVERED |
| AC2 | 5+ categories (programming_languages, frameworks, databases, devops, soft_skills) | `test_taxonomy_has_at_least_five_categories`, `test_each_category_has_skills` | ✓ COVERED |
| AC3 | Each skill has canonical name and aliases | `test_each_skill_has_canonical_and_aliases`, `test_aliases_is_list` | ✓ COVERED |
| AC4 | Create `HOT_SKILLS_2024` dict | `test_hot_skills_has_categories`, `test_hot_skills_exist_in_taxonomy`, `test_is_hot_skill_function` | ✓ COVERED |
| AC5 | Create `skill_extractor.py` with SkillExtractor class | `TestSkillExtractorSingleton` class (3 tests) | ✓ COVERED |
| AC6 | `extract_skills()` returns `Dict[category, List[skill]]` | `TestExtractSkills` class (9 tests) | ✓ COVERED |
| AC7 | `normalize_skill()` normalizes skill names | `TestNormalizeSkill` class (7 tests) | ✓ COVERED |
| AC8 | Basic tests for skill extraction | 63 tests total | ✓ COVERED |

**Coverage Gaps**: NONE - All acceptance criteria have corresponding test coverage.

### Test Architecture Assessment

**Overall: EXCELLENT**

1. **Test Coverage Adequacy**: 63 tests covering:
   - Taxonomy structure validation (25 tests)
   - Skill normalization (7 tests)
   - Skill categorization (3 tests)
   - Skill extraction (9 tests)
   - Special character handling (5 tests)
   - Edge cases (7 tests)
   - Flat extraction (4 tests)
   - Singleton pattern (3 tests)

2. **Test Level Appropriateness**: 
   - ✓ All unit tests - appropriate for internal utility classes
   - ✓ No integration tests needed (no external dependencies or DB)
   - ✓ No E2E tests needed (no API endpoints)

3. **Test Design Quality**:
   - Well-organized test classes by feature
   - Proper use of pytest fixtures (`extractor` fixture with reset)
   - Good test isolation via singleton reset
   - Descriptive test names

4. **Edge Case Coverage**: EXCELLENT
   - Empty input handling
   - Unicode text support (Vietnamese text tested)
   - Special characters (C++, C#, .NET)
   - Markdown formatting (`**Python**`, `_React_`, `` `Docker` ``)
   - Very long text
   - Duplicates
   - Case insensitivity

### Refactoring Performed

No refactoring performed - code quality is already excellent.

### Compliance Check

- **Coding Standards**: ✓ 
  - `snake_case` for functions/variables
  - `PascalCase` for classes
  - Type hints throughout
  - Comprehensive docstrings
- **Project Structure**: ✓ 
  - Files in correct location (`backend/app/modules/ai/`)
  - Tests in correct location (`backend/tests/modules/ai/`)
  - Proper exports in `__init__.py`
- **Testing Strategy**: ✓ 
  - Unit tests for isolated utility classes
  - Co-located test pattern followed
  - Pytest used as framework
- **All ACs Met**: ✓ All 8 acceptance criteria verified

### Improvements Checklist

All items are satisfied - no required changes:

- [x] Proper type annotations (mypy passes)
- [x] Comprehensive docstrings with examples
- [x] Debug logging implemented
- [x] Singleton pattern correctly implemented
- [x] Edge cases handled
- [x] Special characters supported

**Optional Future Enhancements** (not blocking):
- [ ] Consider adding skill weighting/importance scores in future story
- [ ] Consider async version if needed for high-volume processing
- [ ] Consider caching compiled patterns if memory becomes concern

### Security Review

**Status: PASS**

- No security concerns - this is an internal utility module
- No user input directly passed to regex (patterns pre-compiled from taxonomy)
- No external API calls
- No sensitive data handling
- No file system access

### Performance Considerations

**Status: PASS**

- Singleton pattern prevents repeated initialization
- Patterns pre-compiled during initialization
- `O(n*m)` complexity acceptable (n=text length, m=pattern count)
- Set-based deduplication for efficient duplicate handling
- Tested with "very long text" edge case

### Testability Evaluation

| Aspect | Rating | Notes |
|--------|--------|-------|
| Controllability | ✓ HIGH | Pure functions, no external dependencies |
| Observability | ✓ HIGH | Clear return types, debug logging |
| Debuggability | ✓ HIGH | Singleton reset for test isolation |

### Files Modified During Review

None - no modifications made by QA.

### Gate Status

**Gate: PASS** → `docs/qa/gates/5.1-skill-taxonomy-extractor-foundation.yml`

### Recommended Status

**✓ Ready for Done**

All acceptance criteria are met with comprehensive test coverage. Code quality is excellent with no issues found. The implementation provides a solid foundation for the hybrid skill scoring system.

