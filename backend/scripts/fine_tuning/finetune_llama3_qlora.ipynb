{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Fine-tune Llama 3 8B v·ªõi QLoRA cho CV-JD Matching\n",
        "\n",
        "**M·ª•c ƒë√≠ch:** Fine-tune model Llama 3 8B ƒë·ªÉ ƒë√°nh gi√° v√† so kh·ªõp CV v·ªõi JD\n",
        "\n",
        "**Y√™u c·∫ßu:**\n",
        "- Google Colab (Free tier v·ªõi T4 GPU ƒë·ªß d√πng)\n",
        "- Hugging Face account v·ªõi Llama 3 access\n",
        "- Training data t·ª´ preprocessing pipeline\n",
        "\n",
        "**Th·ªùi gian ∆∞·ªõc t√≠nh:** 2-4 gi·ªù cho full training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# C√†i ƒë·∫∑t dependencies\n",
        "!pip install -U transformers datasets accelerate peft bitsandbytes trl\n",
        "!pip install -U huggingface_hub scipy\n",
        "!pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from datetime import datetime\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from datasets import load_dataset, Dataset\n",
        "from trl import SFTTrainer\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Hugging Face Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Nh·∫≠p Hugging Face token c·ªßa b·∫°n\n",
        "# L·∫•y token t·∫°i: https://huggingface.co/settings/tokens\n",
        "HF_TOKEN = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "if HF_TOKEN:\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"‚úÖ Logged in to Hugging Face\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Please enter your Hugging Face token\")\n",
        "    login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# C·∫§U H√åNH - ƒêI·ªÄU CH·ªàNH THEO NHU C·∫¶U\n",
        "# ============================================\n",
        "\n",
        "# Model configuration\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # Base model\n",
        "NEW_MODEL_NAME = \"llama3-8b-cv-jd-matcher\"  # T√™n model sau fine-tune\n",
        "\n",
        "# Training configuration\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 2  # Gi·∫£m n·∫øu h·∫øt memory\n",
        "GRADIENT_ACCUMULATION = 4  # Effective batch = BATCH_SIZE * GRADIENT_ACCUMULATION\n",
        "LEARNING_RATE = 2e-4\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "WARMUP_RATIO = 0.03\n",
        "\n",
        "# LoRA configuration\n",
        "LORA_R = 64  # LoRA rank\n",
        "LORA_ALPHA = 128  # LoRA alpha (th∆∞·ªùng = 2 * r)\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# QLoRA 4-bit quantization\n",
        "USE_4BIT = True\n",
        "BNB_4BIT_COMPUTE_DTYPE = \"float16\"\n",
        "BNB_4BIT_QUANT_TYPE = \"nf4\"\n",
        "USE_NESTED_QUANT = False\n",
        "\n",
        "# Output\n",
        "OUTPUT_DIR = \"./results\"\n",
        "LOGGING_STEPS = 25\n",
        "SAVE_STEPS = 100\n",
        "\n",
        "print(\"üìã Configuration:\")\n",
        "print(f\"   Base Model: {BASE_MODEL}\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   Effective Batch Size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
        "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"   LoRA Rank: {LORA_R}\")\n",
        "print(f\"   Max Sequence Length: {MAX_SEQ_LENGTH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Upload Training Data\n",
        "\n",
        "Upload file `train.jsonl` t·ª´ m√°y local c·ªßa b·∫°n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Option 1: Upload t·ª´ m√°y local\n",
        "print(\"üì§ Upload training data (train.jsonl):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Ki·ªÉm tra file ƒë√£ upload\n",
        "if 'train.jsonl' in uploaded:\n",
        "    print(f\"‚úÖ Uploaded train.jsonl ({len(uploaded['train.jsonl'])/1024:.1f} KB)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Please upload train.jsonl file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 2: Upload t·ª´ Google Drive (n·∫øu file l·ªõn)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !cp \"/content/drive/MyDrive/path/to/train.jsonl\" ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load v√† ki·ªÉm tra dataset\n",
        "def load_jsonl(filepath):\n",
        "    data = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "train_data = load_jsonl('train.jsonl')\n",
        "print(f\"üìä Loaded {len(train_data)} training examples\")\n",
        "\n",
        "# Hi·ªÉn th·ªã sample\n",
        "print(\"\\nüìù Sample:\")\n",
        "sample = train_data[0]\n",
        "print(f\"   Messages: {len(sample['messages'])}\")\n",
        "print(f\"   System: {sample['messages'][0]['content'][:100]}...\")\n",
        "print(f\"   Match Score: {sample['metadata']['match_score']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_chat_template(example):\n",
        "    \"\"\"Format messages into Llama 3 chat template.\"\"\"\n",
        "    messages = example['messages']\n",
        "    \n",
        "    # Llama 3 chat format\n",
        "    formatted = \"\"\n",
        "    for msg in messages:\n",
        "        role = msg['role']\n",
        "        content = msg['content']\n",
        "        \n",
        "        if role == 'system':\n",
        "            formatted += f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
        "        elif role == 'user':\n",
        "            formatted += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
        "        elif role == 'assistant':\n",
        "            formatted += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
        "    \n",
        "    return {'text': formatted}\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "dataset = Dataset.from_list(train_data)\n",
        "dataset = dataset.map(format_chat_template)\n",
        "\n",
        "print(f\"‚úÖ Dataset prepared: {len(dataset)} examples\")\n",
        "print(f\"   Sample text length: {len(dataset[0]['text'])} chars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load Model v·ªõi QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quantization config cho 4-bit\n",
        "compute_dtype = getattr(torch, BNB_4BIT_COMPUTE_DTYPE)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=USE_4BIT,\n",
        "    bnb_4bit_quant_type=BNB_4BIT_QUANT_TYPE,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=USE_NESTED_QUANT,\n",
        ")\n",
        "\n",
        "print(\"üì• Loading base model (this may take a few minutes)...\")\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=compute_dtype,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "print(f\"‚úÖ Model loaded!\")\n",
        "print(f\"   Model size: {model.get_memory_footprint() / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"‚úÖ Tokenizer loaded!\")\n",
        "print(f\"   Vocab size: {tokenizer.vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Configure LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    r=LORA_R,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=SAVE_STEPS,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.001,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"none\",  # Disable wandb\n",
        ")\n",
        "\n",
        "print(\"üìã Training Arguments:\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
        "print(f\"   Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
        "print(f\"   Learning rate: {LEARNING_RATE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer initialized!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ START TRAINING\n",
        "print(\"=\"*60)\n",
        "print(\"üöÄ STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "print(f\"   Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"   Estimated duration: 2-4 hours\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Train!\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save LoRA adapters\n",
        "ADAPTER_PATH = f\"./adapters/{NEW_MODEL_NAME}\"\n",
        "trainer.model.save_pretrained(ADAPTER_PATH)\n",
        "tokenizer.save_pretrained(ADAPTER_PATH)\n",
        "\n",
        "print(f\"‚úÖ LoRA adapters saved to: {ADAPTER_PATH}\")\n",
        "!ls -la {ADAPTER_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download adapters to local machine\n",
        "!zip -r adapters.zip ./adapters\n",
        "files.download('adapters.zip')\n",
        "print(\"üì• Downloading adapters.zip...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test prompt\n",
        "test_cv = \"\"\"\n",
        "**M·ª•c ti√™u ngh·ªÅ nghi·ªáp:** K·ªπ s∆∞ ph·∫ßn m·ªÅm v·ªõi 3 nƒÉm kinh nghi·ªám, mong mu·ªën ph√°t tri·ªÉn trong lƒ©nh v·ª±c AI/ML\n",
        "**K·ªπ nƒÉng:** Python, TensorFlow, PyTorch, SQL, Docker, Git\n",
        "**H·ªçc v·∫•n:** C·ª≠ nh√¢n Khoa h·ªçc M√°y t√≠nh - ƒê·∫°i h·ªçc B√°ch khoa H√† N·ªôi (2020)\n",
        "**Kinh nghi·ªám:** ML Engineer t·∫°i FPT Software (2020-2023)\n",
        "\"\"\"\n",
        "\n",
        "test_jd = \"\"\"\n",
        "**V·ªã tr√≠:** Senior Machine Learning Engineer\n",
        "**Y√™u c·∫ßu h·ªçc v·∫•n:** C·ª≠ nh√¢n CNTT ho·∫∑c t∆∞∆°ng ƒë∆∞∆°ng\n",
        "**Y√™u c·∫ßu kinh nghi·ªám:** √çt nh·∫•t 2 nƒÉm kinh nghi·ªám ML/AI\n",
        "**K·ªπ nƒÉng y√™u c·∫ßu:** Python, TensorFlow/PyTorch, MLOps, Kubernetes\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "B·∫°n l√† chuy√™n gia ƒë√°nh gi√° v√† so kh·ªõp CV v·ªõi Job Description (JD). \n",
        "Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch m·ª©c ƒë·ªô ph√π h·ª£p gi·ªØa CV c·ªßa ·ª©ng vi√™n v√† y√™u c·∫ßu c√¥ng vi·ªác.\n",
        "Lu√¥n tr·∫£ l·ªùi b·∫±ng JSON format.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Ph√¢n t√≠ch m·ª©c ƒë·ªô ph√π h·ª£p gi·ªØa CV v√† JD sau:\n",
        "\n",
        "## CV ·ª®ng vi√™n:\n",
        "{test_cv}\n",
        "\n",
        "## Y√™u c·∫ßu C√¥ng vi·ªác (JD):\n",
        "{test_jd}\n",
        "\n",
        "H√£y ƒë√°nh gi√° v√† tr·∫£ l·ªùi b·∫±ng JSON.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"üß™ Testing model...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate response\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Extract assistant response\n",
        "assistant_response = response.split(\"assistant\")[-1].strip()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìù MODEL RESPONSE:\")\n",
        "print(\"=\"*60)\n",
        "print(assistant_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. (Optional) Merge & Export to GGUF\n",
        "\n",
        "ƒê·ªÉ s·ª≠ d·ª•ng v·ªõi Ollama, c·∫ßn merge LoRA v√†o base model v√† export sang GGUF format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge LoRA v·ªõi base model (c·∫ßn nhi·ªÅu RAM)\n",
        "# Ch·ªâ ch·∫°y n·∫øu c√≥ ƒë·ªß RAM (>16GB)\n",
        "\n",
        "MERGE_MODEL = False  # Set True ƒë·ªÉ merge\n",
        "\n",
        "if MERGE_MODEL:\n",
        "    print(\"üîÑ Merging LoRA adapters with base model...\")\n",
        "    \n",
        "    # Reload model in FP16\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        BASE_MODEL,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    \n",
        "    # Merge\n",
        "    merged_model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "    merged_model = merged_model.merge_and_unload()\n",
        "    \n",
        "    # Save merged model\n",
        "    MERGED_PATH = f\"./merged/{NEW_MODEL_NAME}\"\n",
        "    merged_model.save_pretrained(MERGED_PATH)\n",
        "    tokenizer.save_pretrained(MERGED_PATH)\n",
        "    \n",
        "    print(f\"‚úÖ Merged model saved to: {MERGED_PATH}\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è Skipping merge (set MERGE_MODEL=True to enable)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Push to Hugging Face Hub (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PUSH_TO_HUB = False  # Set True ƒë·ªÉ push\n",
        "HUB_MODEL_ID = \"your-username/llama3-8b-cv-jd-matcher\"  # Thay b·∫±ng username c·ªßa b·∫°n\n",
        "\n",
        "if PUSH_TO_HUB:\n",
        "    print(f\"üì§ Pushing to Hugging Face Hub: {HUB_MODEL_ID}\")\n",
        "    trainer.model.push_to_hub(HUB_MODEL_ID)\n",
        "    tokenizer.push_to_hub(HUB_MODEL_ID)\n",
        "    print(\"‚úÖ Pushed successfully!\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è Skipping push (set PUSH_TO_HUB=True to enable)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéâ Ho√†n th√†nh!\n",
        "\n",
        "### C√°c b∆∞·ªõc ti·∫øp theo:\n",
        "\n",
        "1. **Download adapters.zip** v·ªÅ m√°y local\n",
        "2. **Merge v·ªõi base model** (n·∫øu ch∆∞a l√†m)\n",
        "3. **Convert sang GGUF** ƒë·ªÉ d√πng v·ªõi Ollama\n",
        "4. **Deploy** v√†o ·ª©ng d·ª•ng\n",
        "\n",
        "### ƒê·ªÉ convert sang GGUF v√† d√πng v·ªõi Ollama:\n",
        "\n",
        "```bash\n",
        "# Clone llama.cpp\n",
        "git clone https://github.com/ggerganov/llama.cpp\n",
        "cd llama.cpp\n",
        "\n",
        "# Convert to GGUF\n",
        "python convert.py /path/to/merged/model --outfile model.gguf\n",
        "\n",
        "# Quantize (optional, reduces size)\n",
        "./quantize model.gguf model-q4_0.gguf q4_0\n",
        "\n",
        "# Create Ollama Modelfile\n",
        "echo 'FROM ./model-q4_0.gguf' > Modelfile\n",
        "\n",
        "# Create Ollama model\n",
        "ollama create cv-jd-matcher -f Modelfile\n",
        "\n",
        "# Test\n",
        "ollama run cv-jd-matcher\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
