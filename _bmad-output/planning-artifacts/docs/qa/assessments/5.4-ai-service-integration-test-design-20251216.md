# Test Design: Story 5.4 - AI Service Integration

**Date:** 2025-12-16  
**Designer:** Quinn (Test Architect)  
**Story ID:** 5.4  
**Story Title:** AI Service Integration

## Test Strategy Overview

- **Total test scenarios:** 18
- **Unit tests:** 13 (72%)
- **Integration tests:** 5 (28%)
- **E2E tests:** 0 (0%)
- **Priority distribution:** P0: 9, P1: 7, P2: 2

### Rationale for Test Level Distribution

This story focuses on integrating the `SkillScorer` into the existing `AIService` pipeline. The distribution emphasizes unit tests because:

1. **Pure logic integration:** Most complexity lies in merging scorer results with LLM outputs
2. **Testability:** Component boundaries are clear and mockable
3. **Fast feedback:** Integration points can be validated via mocks without full infrastructure
4. **Existing infrastructure:** Database and schema updates were completed in Story 5.3

Integration tests are included for critical persistence and error handling paths. E2E tests are unnecessary as the integration is internal to the AI pipeline with no user-facing changes.

## Test Scenarios by Acceptance Criteria

### AC1: Update `AIService.__init__()` to initialize `SkillScorer`

#### Scenarios

| ID            | Level       | Priority | Test Description                                  | Justification                                                  |
| ------------- | ----------- | -------- | ------------------------------------------------- | -------------------------------------------------------------- |
| 5.4-UNIT-001  | Unit        | P0       | Verify skill_scorer is initialized in constructor | Critical initialization path; pure object construction         |
| 5.4-UNIT-002  | Unit        | P1       | Verify skill_scorer is singleton instance         | Validates singleton pattern usage; resource efficiency concern |

### AC2: Update `_perform_ai_analysis()` to call skill scorer

#### Scenarios

| ID            | Level       | Priority | Test Description                                                      | Justification                                                         |
| ------------- | ----------- | -------- | --------------------------------------------------------------------- | --------------------------------------------------------------------- |
| 5.4-UNIT-003  | Unit        | P0       | Verify skill scorer called with correct CV text                       | Core integration point; validates data flow                           |
| 5.4-UNIT-004  | Unit        | P0       | Verify skill scorer called with LLM response                          | Ensures hybrid scoring receives both inputs                           |
| 5.4-UNIT-005  | Unit        | P0       | Verify skill scorer results merged into analysis results              | Critical data transformation; must preserve all fields                |
| 5.4-INT-001   | Integration | P0       | Test complete analysis pipeline with real scorer                      | Validates end-to-end integration without mocks                        |
| 5.4-UNIT-006  | Unit        | P1       | Verify existing Ollama call remains unchanged                         | Backward compatibility; prevents regression                           |
| 5.4-UNIT-007  | Unit        | P1       | Verify existing JSON parsing preserved                                | Backward compatibility; protects current output format                |

### AC3: Extract evidence score from LLM response

#### Scenarios

| ID            | Level       | Priority | Test Description                                                      | Justification                                                         |
| ------------- | ----------- | -------- | --------------------------------------------------------------------- | --------------------------------------------------------------------- |
| 5.4-UNIT-008  | Unit        | P0       | Test evidence score normalization (0-100 → 0-6)                       | Critical calculation; must be mathematically correct                  |
| 5.4-UNIT-009  | Unit        | P0       | Test evidence score clamping (values outside 0-6 range)               | Edge case handling; prevents data corruption                          |
| 5.4-UNIT-010  | Unit        | P1       | Test default evidence score when LLM response missing                 | Graceful degradation; ensures system resilience                       |

### AC4: Merge `skill_breakdown` into analysis results

#### Scenarios

| ID            | Level       | Priority | Test Description                                                      | Justification                                                         |
| ------------- | ----------- | -------- | --------------------------------------------------------------------- | --------------------------------------------------------------------- |
| 5.4-UNIT-011  | Unit        | P0       | Verify all skill_breakdown fields present in results                  | Data integrity; must include all scoring components                   |
| 5.4-UNIT-012  | Unit        | P1       | Verify skill_categories structure in results                          | Validates correct nested structure for categorization                 |
| 5.4-UNIT-013  | Unit        | P1       | Verify skill_recommendations list in results                          | Ensures recommendations properly propagated                           |

### AC5: Update `_save_analysis_results()` to save new columns

#### Scenarios

| ID            | Level       | Priority | Test Description                                                      | Justification                                                         |
| ------------- | ----------- | -------- | --------------------------------------------------------------------- | --------------------------------------------------------------------- |
| 5.4-INT-002   | Integration | P0       | Test persistence of skill_breakdown JSONB column                      | Critical database operation; validates schema integration             |
| 5.4-INT-003   | Integration | P0       | Test persistence of skill_categories JSONB column                     | Critical database operation; must store complex nested data           |
| 5.4-INT-004   | Integration | P1       | Test persistence of skill_recommendations JSONB column                | Secondary field persistence validation                                |

### AC6: Existing CV analysis flow not broken

#### Scenarios

| ID            | Level       | Priority | Test Description                                                      | Justification                                                         |
| ------------- | ----------- | -------- | --------------------------------------------------------------------- | --------------------------------------------------------------------- |
| 5.4-INT-005   | Integration | P0       | Test backward compatibility - existing fields unchanged               | Prevents regression; critical for production stability                |
| 5.4-UNIT-014  | Unit        | P0       | Test fallback analysis includes skill fields with safe defaults       | Resilience; prevents runtime errors on AI service failure             |
| 5.4-UNIT-015  | Unit        | P1       | Test skill scorer failure handled gracefully                          | Error handling; ensures system degrades gracefully                    |

### AC7: Logging skill extraction and scoring results

#### Scenarios

| ID            | Level       | Priority | Test Description                                                      | Justification                                                         |
| ------------- | ----------- | -------- | --------------------------------------------------------------------- | --------------------------------------------------------------------- |
| 5.4-UNIT-016  | Unit        | P2       | Verify INFO logging for scoring summary                               | Observability; helps debugging but not critical to functionality      |
| 5.4-UNIT-017  | Unit        | P2       | Verify WARNING logging when LLM evidence missing                      | Observability; alerts to potential issues                             |

## Existing Test Coverage (Story 5.4 Implementation)

The following tests were already implemented during Story 5.4 development:

### Implemented Tests in `backend/tests/modules/ai/test_ai_service.py`

| Test Name                                          | Maps to Scenario(s)                  | Coverage Notes                                      |
| -------------------------------------------------- | ------------------------------------ | --------------------------------------------------- |
| `test_fallback_analysis_includes_skill_fields`     | 5.4-UNIT-014                         | ✅ Validates fallback with safe defaults            |
| `test_perform_ai_analysis_integrates_skill_scorer` | 5.4-UNIT-003, 5.4-UNIT-004, 5.4-UNIT-005, 5.4-UNIT-011, 5.4-UNIT-012, 5.4-UNIT-013 | ✅ Comprehensive integration validation |
| `test_skill_scorer_failure_uses_fallback`          | 5.4-UNIT-015                         | ✅ Error handling and graceful degradation          |
| `test_save_analysis_results_persists_skill_fields` | 5.4-INT-002, 5.4-INT-003, 5.4-INT-004 | ⚠️ Partial - validates method call, not DB state   |

### Coverage Analysis

**Strong Coverage (P0):**
- ✅ Skill scorer integration (5.4-UNIT-003, 5.4-UNIT-004, 5.4-UNIT-005)
- ✅ Fallback handling (5.4-UNIT-014)
- ✅ Error resilience (5.4-UNIT-015)
- ⚠️ Persistence validation (limited - see gaps below)

**Coverage Gaps Identified:**

1. **P0 Gaps:**
   - 5.4-UNIT-001: Constructor initialization not explicitly tested
   - 5.4-UNIT-008: Evidence score normalization not isolated
   - 5.4-UNIT-009: Edge case clamping not tested
   - 5.4-INT-001: Full pipeline without mocks not tested
   - 5.4-INT-002/003: Database state validation incomplete
   - 5.4-INT-005: Backward compatibility not comprehensively verified

2. **P1 Gaps:**
   - 5.4-UNIT-002: Singleton pattern not verified
   - 5.4-UNIT-006: Ollama call preservation not tested
   - 5.4-UNIT-007: JSON parsing preservation not tested
   - 5.4-UNIT-010: Default evidence score behavior not isolated
   - 5.4-INT-004: Recommendations persistence not verified

3. **P2 Gaps:**
   - 5.4-UNIT-016: Logging verification not implemented
   - 5.4-UNIT-017: Warning logging not tested

## Recommended Additional Tests

### High Priority (P0) - Address These First

```python
# 5.4-UNIT-001: Test constructor initialization
def test_skill_scorer_initialization(self, ai_service):
    """Verify skill_scorer is properly initialized in AIService constructor."""
    assert hasattr(ai_service, 'skill_scorer')
    assert ai_service.skill_scorer is not None
    from app.modules.ai.skill_scorer import SkillScorer
    assert isinstance(ai_service.skill_scorer, SkillScorer)

# 5.4-UNIT-008: Test evidence score normalization
def test_evidence_score_normalization(self, ai_service):
    """Test normalization formula: round(value * 6 / 100) clamped to 0-6."""
    test_cases = [
        (0, 0),      # Min value
        (50, 3),     # Mid value
        (75, 5),     # High value  
        (100, 6),    # Max value
        (83, 5),     # Round down
        (84, 5),     # Round up
    ]
    for llm_value, expected_score in test_cases:
        result = round(llm_value * 6 / 100)
        result = max(0, min(6, result))
        assert result == expected_score

# 5.4-UNIT-009: Test evidence score clamping
def test_evidence_score_clamping(self, ai_service):
    """Test clamping for out-of-range values."""
    test_cases = [
        (-10, 0),   # Negative value
        (150, 6),   # Above max
        (0, 0),     # At min boundary
        (100, 6),   # At max boundary
    ]
    for llm_value, expected_score in test_cases:
        result = round(llm_value * 6 / 100)
        result = max(0, min(6, result))
        assert result == expected_score

# 5.4-INT-001: Integration test with real scorer (no mocks)
@pytest.mark.asyncio
async def test_complete_analysis_pipeline_integration(self, ai_service):
    """Test full analysis pipeline with real SkillScorer instance."""
    cv_content = "Senior Python Developer with 5 years React and PostgreSQL experience"
    
    with patch('app.modules.ai.service.rag_service') as mock_rag:
        mock_rag.retrieve_context.return_value = []
        
        with patch.object(ai_service, '_call_ollama') as mock_ollama:
            mock_ollama.return_value = '''
            {
                "score": 85,
                "criteria": {"skills": 80},
                "summary": "Strong technical skills",
                "skills": ["Python", "React", "PostgreSQL"],
                "experience_breakdown": {},
                "strengths": [],
                "improvements": [],
                "formatting_feedback": [],
                "ats_hints": []
            }
            '''
            
            result = await ai_service._perform_ai_analysis(cv_content)
            
            # Verify integration without mocking skill_scorer
            assert "skill_breakdown" in result
            assert result["skill_breakdown"]["total_score"] > 0
            assert "skill_categories" in result
            assert len(result["skill_categories"]) > 0
            assert "skill_recommendations" in result

# 5.4-INT-002/003/004: Enhanced persistence test with DB verification
@pytest.mark.asyncio
async def test_skill_fields_persisted_to_database(self, ai_service, async_session):
    """Test that new JSONB columns are actually written to database."""
    from app.modules.ai.models import CVAnalysis
    import uuid
    
    cv_id = uuid.uuid4()
    
    # Create a CV analysis record first
    cv_analysis = CVAnalysis(
        id=cv_id,
        cv_id=cv_id,
        ai_score=0,
        ai_summary="",
        ai_feedback={}
    )
    async_session.add(cv_analysis)
    await async_session.commit()
    
    results = {
        "score": 85,
        "summary": "Test",
        "criteria": {},
        "skills": ["Python"],
        "experience_breakdown": {},
        "strengths": [],
        "improvements": [],
        "formatting_feedback": [],
        "ats_hints": [],
        "skill_breakdown": {
            "completeness_score": 6,
            "categorization_score": 5,
            "evidence_score": 4,
            "market_relevance_score": 5,
            "total_score": 20
        },
        "skill_categories": {
            "programming_languages": ["Python", "JavaScript"]
        },
        "skill_recommendations": ["Learn cloud platforms"]
    }
    
    await ai_service._save_analysis_results(async_session, cv_id, results)
    
    # Query back from database
    from sqlalchemy import select
    stmt = select(CVAnalysis).where(CVAnalysis.id == cv_id)
    result = await async_session.execute(stmt)
    persisted = result.scalar_one()
    
    # Verify JSONB columns
    assert persisted.skill_breakdown is not None
    assert persisted.skill_breakdown["total_score"] == 20
    assert persisted.skill_categories is not None
    assert "programming_languages" in persisted.skill_categories
    assert persisted.skill_recommendations is not None
    assert len(persisted.skill_recommendations) > 0

# 5.4-INT-005: Backward compatibility test
@pytest.mark.asyncio
async def test_backward_compatibility_existing_fields(self, ai_service):
    """Verify all existing fields remain unchanged after integration."""
    cv_content = "Test CV"
    
    with patch('app.modules.ai.service.rag_service') as mock_rag:
        mock_rag.retrieve_context.return_value = []
        
        with patch.object(ai_service, '_call_ollama') as mock_ollama:
            mock_ollama.return_value = '''
            {
                "score": 75,
                "criteria": {"completeness": 80, "experience": 70, "skills": 75, "professionalism": 75},
                "summary": "Good candidate",
                "skills": ["Python", "JavaScript"],
                "experience_breakdown": {"total_years": 3},
                "strengths": ["Strong coding"],
                "improvements": ["Add certifications"],
                "formatting_feedback": ["Good structure"],
                "ats_hints": ["Use keywords"]
            }
            '''
            
            result = await ai_service._perform_ai_analysis(cv_content)
            
            # Verify all legacy fields present and unchanged
            assert result["score"] == 75
            assert "summary" in result
            assert "criteria" in result
            assert "skills" in result
            assert isinstance(result["skills"], list)
            assert "experience_breakdown" in result
            assert "strengths" in result
            assert "improvements" in result
            assert "formatting_feedback" in result
            assert "ats_hints" in result
            
            # Verify new fields also present
            assert "skill_breakdown" in result
            assert "skill_categories" in result
            assert "skill_recommendations" in result
```

### Medium Priority (P1) - Address After P0

```python
# 5.4-UNIT-002: Singleton pattern verification
def test_skill_scorer_is_singleton(self, ai_service):
    """Verify skill_scorer uses singleton instance."""
    from app.modules.ai.skill_scorer import skill_scorer
    assert ai_service.skill_scorer is skill_scorer

# 5.4-UNIT-010: Default evidence score test
@pytest.mark.asyncio
async def test_default_evidence_score_when_missing(self, ai_service):
    """Test default evidence score of 3 when LLM response missing skills criteria."""
    cv_content = "Test CV"
    
    with patch.object(ai_service.skill_scorer, 'calculate_skill_score') as mock_scorer:
        mock_scorer.return_value = {
            "completeness_score": 5,
            "categorization_score": 4,
            "evidence_score": 3,  # Default
            "market_relevance_score": 5,
            "total_score": 17,
            "skill_categories": {},
            "recommendations": []
        }
        
        with patch('app.modules.ai.service.rag_service') as mock_rag:
            mock_rag.retrieve_context.return_value = []
            
            with patch.object(ai_service, '_call_ollama') as mock_ollama:
                # Response WITHOUT skills criteria
                mock_ollama.return_value = '''
                {
                    "score": 70,
                    "criteria": {"completeness": 75},
                    "summary": "OK",
                    "skills": [],
                    "experience_breakdown": {},
                    "strengths": [],
                    "improvements": [],
                    "formatting_feedback": [],
                    "ats_hints": []
                }
                '''
                
                result = await ai_service._perform_ai_analysis(cv_content)
                
                # Verify default was used
                assert result["skill_breakdown"]["evidence_score"] == 3
```

### Low Priority (P2) - Nice to Have

```python
# 5.4-UNIT-016: Test INFO logging
def test_skill_scoring_info_logging(self, ai_service, caplog):
    """Verify INFO level logging for skill scoring results."""
    import logging
    caplog.set_level(logging.INFO)
    
    # Trigger analysis that logs skill scoring
    # ... test implementation
    
    assert any("skill scoring" in record.message.lower() for record in caplog.records)

# 5.4-UNIT-017: Test WARNING logging
def test_missing_evidence_warning_logging(self, ai_service, caplog):
    """Verify WARNING logged when LLM evidence score missing."""
    import logging
    caplog.set_level(logging.WARNING)
    
    # Trigger analysis with missing evidence
    # ... test implementation
    
    assert any("evidence" in record.message.lower() for record in caplog.records)
```

## Risk Coverage

Based on Story 5.4 scope, the primary risks and their test coverage:

| Risk ID | Risk Description                                | Mitigation Tests                                    |
| ------- | ----------------------------------------------- | --------------------------------------------------- |
| R-5.4-1 | Skill scorer integration breaks existing flow   | 5.4-INT-005, 5.4-UNIT-006, 5.4-UNIT-007            |
| R-5.4-2 | Evidence score calculation incorrect            | 5.4-UNIT-008, 5.4-UNIT-009, 5.4-UNIT-010           |
| R-5.4-3 | Database persistence fails for new columns      | 5.4-INT-002, 5.4-INT-003, 5.4-INT-004              |
| R-5.4-4 | Skill scorer failure crashes analysis pipeline  | 5.4-UNIT-015, 5.4-UNIT-014                         |
| R-5.4-5 | Merged results missing required fields          | 5.4-UNIT-005, 5.4-UNIT-011, 5.4-UNIT-012           |

## Recommended Execution Order

### Phase 1: Critical Path (P0 Tests)
1. Constructor and initialization tests (5.4-UNIT-001)
2. Evidence score calculation tests (5.4-UNIT-008, 5.4-UNIT-009)
3. Integration tests (5.4-UNIT-003, 5.4-UNIT-004, 5.4-UNIT-005)
4. Error handling tests (5.4-UNIT-014, 5.4-UNIT-015)
5. Database persistence tests (5.4-INT-002, 5.4-INT-003, 5.4-INT-005)
6. Full pipeline integration (5.4-INT-001)

### Phase 2: Core Functionality (P1 Tests)
7. Singleton pattern (5.4-UNIT-002)
8. Backward compatibility (5.4-UNIT-006, 5.4-UNIT-007)
9. Default behavior (5.4-UNIT-010)
10. Field structure validation (5.4-UNIT-012, 5.4-UNIT-013)

### Phase 3: Observability (P2 Tests)
11. Logging tests (5.4-UNIT-016, 5.4-UNIT-017)

## Test Environment Requirements

### Unit Tests
- **Framework:** pytest
- **Dependencies:** pytest-asyncio, pytest-mock
- **Fixtures Required:**
  - `ai_service`: AIService instance with mocked dependencies
  - Standard mocking for `_call_ollama`, `rag_service`

### Integration Tests
- **Framework:** pytest with async support
- **Dependencies:** 
  - `AsyncSession` fixture with test database
  - Database migration applied (Story 5.3 schema)
- **Fixtures Required:**
  - `async_session`: Real database session
  - `test_db`: Test database with schema
  - Database cleanup between tests

### Test Data Requirements
- Sample CV content with varying skill levels
- Mock LLM responses with different `criteria.skills` values (0, 50, 75, 100, 150, -10)
- Sample skill scorer results with complete data structures

## Quality Checklist

- [x] Every AC has test coverage (all 7 ACs covered)
- [x] Test levels are appropriate (72% unit, 28% integration - efficient distribution)
- [x] No duplicate coverage across levels (scenarios clearly separated)
- [x] Priorities align with business risk (P0: critical paths, P1: compatibility, P2: logging)
- [x] Test IDs follow naming convention (5.4-{LEVEL}-{SEQ})
- [x] Scenarios are atomic and independent (each test validates single concern)
- [x] Existing tests mapped to scenarios (4 tests already implemented)
- [x] Coverage gaps identified and prioritized

## Summary

This test design provides comprehensive coverage for Story 5.4's AI Service Integration with an efficient test distribution favoring fast, isolated unit tests while using integration tests strategically for database and backward compatibility validation.

**Key Strengths:**
- 9 P0 tests ensure critical functionality protected
- 7 P1 tests validate compatibility and edge cases
- Existing 4 tests cover major integration points
- Clear gaps identified with recommended implementations

**Recommended Next Steps:**
1. Implement the 6 missing P0 tests first (especially evidence score normalization and database verification)
2. Add P1 tests for singleton pattern and backward compatibility
3. Consider P2 logging tests if observability is a concern

**Total Test Effort Estimate:**
- Existing: 4 tests (22% of design)
- P0 Gaps: 6 tests (~3-4 hours)
- P1 Gaps: 5 tests (~2-3 hours)
- P2 Gaps: 2 tests (~1 hour)
- **Total additional effort:** ~6-8 hours for complete coverage
