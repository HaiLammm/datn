# Story 3.2: JD Parsing & Skill Extraction

## Status
Done

## Story
**As a** system,
**I want** to parse JD content and extract requirements,
**So that** matching can be performed accurately between JD requirements and CV skills.

## Acceptance Criteria

1. Create `JDParser` class in `backend/app/modules/jobs/jd_parser.py` use LLM (Ollama)
2. Extract `required_skills` tu JD text va normalize su dung existing skill taxonomy (`skill_extractor.py`)
3. Extract `min_experience_years` tu JD text (e.g., "3+ years experience" -> 3)
4. Extract `nice_to_have_skills` (optional skills) tu JD text
5. Normalize skills su dung `SkillExtractor` class tu `backend/app/modules/ai/skill_extractor.py`
6. Store parsed results trong `parsed_requirements` JSONB field cua `JobDescription` model
7. Parsing chay async sau khi JD duoc upload (trigger tu service layer)
8. API endpoint update: POST `/api/v1/jobs/jd` nen trigger parsing after create
9. Tao endpoint GET `/api/v1/jobs/jd/{jd_id}/parse-status` de check parsing status
10. Unit tests cho JDParser class va integration tests cho parsing flow

## Tasks / Subtasks

- [x] **Task 1: Create JDParser Class (AC: 1, 2, 3, 4)**
    - [x] Create file `backend/app/modules/jobs/jd_parser.py`
    - [x] Define `JDParser` class with `__init__` that initializes Ollama client URL
    - [x] Implement `parse_jd(jd_text: str) -> ParsedJDRequirements` method
    - [x] Build LLM prompt to extract: required_skills, nice_to_have_skills, min_experience_years, job_title_normalized, key_responsibilities
    - [x] Implement `_parse_llm_response(response: str) -> Dict` with JSON parsing and fallback
    - [x] Implement async `_call_ollama(prompt: str) -> str` using httpx (follow pattern from `ai/service.py`)

- [x] **Task 2: Integrate Skill Taxonomy for Normalization (AC: 5)**
    - [x] Import `SkillExtractor` from `backend/app/modules/ai/skill_extractor.py`
    - [x] Implement `_normalize_skills(raw_skills: List[str]) -> List[str]` method
    - [x] For each skill from LLM, try to normalize using `skill_extractor.normalize_skill()`
    - [x] Keep unrecognized skills as-is (lowercase) to support custom JD requirements
    - [x] Categorize skills using `skill_extractor.get_skill_category()` for future use

- [x] **Task 3: Create Pydantic Schemas for Parsed JD (AC: 6, 9)**
    - [x] Update `backend/app/modules/jobs/schemas.py`
    - [x] Create `ParsedJDRequirements` schema
    - [x] Create `JDParseStatus` enum: PENDING, PROCESSING, COMPLETED, FAILED
    - [x] Create `JDParseStatusResponse` schema for parse-status endpoint
    - [x] Update `JobDescriptionResponse` to include `parsed_requirements` and `parse_status`

- [x] **Task 4: Update JobDescription Model (AC: 6)**
    - [x] Add `parse_status` column to `JobDescription` model (String, default 'pending')
    - [x] Create Alembic migration: `a1b2c3d4e5f6_add_parse_status_to_job_descriptions.py`
    - [x] Verify `parsed_requirements` JSONB field already exists from Story 3.1
    - [x] Run migration: `alembic upgrade head`

- [x] **Task 5: Implement Async Parsing in Service Layer (AC: 7)**
    - [x] Update `backend/app/modules/jobs/services.py`
    - [x] Create `parse_job_description(db: AsyncSession, jd_id: UUID) -> None` function
    - [x] Implement status update flow: PENDING -> PROCESSING -> COMPLETED/FAILED
    - [x] Save parsed results to `parsed_requirements` field
    - [x] Add error handling with logging

- [x] **Task 6: Trigger Parsing After JD Creation (AC: 8)**
    - [x] Update router to use `BackgroundTasks` from FastAPI for async execution
    - [x] Create `_run_jd_parsing` background task wrapper
    - [x] Ensure immediate response to user (parsing runs in background)

- [x] **Task 7: Create Parse Status Endpoint (AC: 9)**
    - [x] Add GET `/api/v1/jobs/jd/{jd_id}/parse-status` endpoint in router.py
    - [x] Add POST `/api/v1/jobs/jd/{jd_id}/reparse` endpoint for re-parsing
    - [x] Use `JDParseStatusResponse` schema for response
    - [x] Auth required, ownership check

- [x] **Task 8: Write Unit Tests (AC: 10)**
    - [x] Create `backend/tests/modules/jobs/test_jd_parser.py` (22 tests)
    - [x] Update `backend/tests/modules/jobs/test_job_service.py` (added 8 tests)
    - [x] Update `backend/tests/modules/jobs/test_job_router.py` (added 8 tests)
    - [x] All 61 tests passing

## Dev Notes

### Previous Story Insights (Story 3.1)

From Story 3.1 implementation:
- `JobDescription` model already has `parsed_requirements` JSONB field (ready for use)
- Jobs module follows pattern: `router.py`, `services.py`, `schemas.py`, `models.py`
- Authentication uses `get_current_user` dependency
- CRUD operations pattern established in `services.py`
- User ownership check pattern: `JobDescription.user_id == user_id`

**[Source: docs/stories/3.1.story.md#Dev-Agent-Record]**

### LLM Integration Pattern

Existing LLM integration in `backend/app/modules/ai/service.py`:
```python
async def _call_ollama(self, prompt: str) -> str:
    async with httpx.AsyncClient(timeout=dynamic_timeout) as client:
        response = await client.post(
            f"{self.ollama_url}/api/generate",
            json={
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "seed": 42,
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "num_predict": 2048,
                }
            }
        )
        return result.get("response", "")
```

**Configuration:**
- Ollama URL: `settings.OLLAMA_URL` (default: "http://localhost:11434")
- Model: `settings.LLM_MODEL` (default: "phi3:latest" or "llama3.1:8b")

**[Source: backend/app/modules/ai/service.py:1058-1102]**

### Skill Normalization Pattern

Use existing `SkillExtractor` for skill normalization:
```python
from app.modules.ai.skill_extractor import skill_extractor

# Normalize a single skill
canonical = skill_extractor.normalize_skill("ReactJS")  # Returns "react"

# Get skill category
category = skill_extractor.get_skill_category("python")  # Returns "programming_languages"

# Extract all skills from text
skills_by_category = skill_extractor.extract_skills(jd_text)
# Returns: {"programming_languages": ["python", "javascript"], "frameworks": ["react"], ...}
```

**[Source: backend/app/modules/ai/skill_extractor.py:140-253]**

### JD Parsing LLM Prompt Template

Suggested prompt structure:
```
Analyze this Job Description and extract structured requirements.

JOB DESCRIPTION:
{jd_text}

Respond with JSON only:
{
    "required_skills": ["skill1", "skill2"],
    "nice_to_have_skills": ["skill1"],
    "min_experience_years": <number or null>,
    "job_title_normalized": "string",
    "key_responsibilities": ["resp1", "resp2"]
}

IMPORTANT:
- Extract ALL technical skills mentioned (programming languages, frameworks, tools)
- For experience, extract the minimum years required (e.g., "3-5 years" -> 3)
- Separate required vs nice-to-have/preferred skills
- Normalize job title (e.g., "Sr. Python Dev" -> "Senior Python Developer")

JSON only:
```

### File Locations

**Files to Create:**
```
backend/app/modules/jobs/
└── jd_parser.py           # NEW: JDParser class

backend/tests/modules/jobs/
└── test_jd_parser.py      # NEW: Parser unit tests
```

**Files to Modify:**
```
backend/app/modules/jobs/
├── models.py              # Add parse_status, parse_error columns
├── schemas.py             # Add ParsedJDRequirements, JDParseStatus, JDParseStatusResponse
├── services.py            # Add parse_job_description(), update create flow
└── router.py              # Add /parse-status endpoint, BackgroundTasks

backend/tests/modules/jobs/
├── test_job_service.py    # Add parsing tests
└── test_job_router.py     # Add parse-status endpoint tests

backend/alembic/versions/
└── xxx_add_parse_status_to_job_descriptions.py  # NEW migration
```

**[Source: docs/architecture/source-tree.md]**

### Technical Constraints

1. **Async Pattern:**
   - Use `BackgroundTasks` from FastAPI for non-blocking parsing
   - Parsing should NOT block the create JD response
   - User gets immediate 201 response, parsing happens async

2. **Error Handling:**
   - LLM timeout: Set reasonable timeout (60-120 seconds)
   - Parse failure: Mark status as FAILED, log error, keep JD with null parsed_requirements
   - Invalid JSON from LLM: Use regex fallback to extract partial data

3. **Skill Normalization:**
   - Always normalize using skill_extractor first
   - Keep unrecognized skills (custom requirements like "internal-tool-x")
   - Store both raw LLM output and normalized skills

4. **Database:**
   - `parse_status` should be VARCHAR(20), not enum (for flexibility)
   - `parsed_requirements` already JSONB from Story 3.1

**[Source: docs/architecture/coding-standards.md]**

### Dependencies

- **Story 3.2 depends on:**
  - Story 3.1 (JD Module Foundation) - **DONE**
  - Epic 5 Skill Taxonomy - skill_taxonomy.py, skill_extractor.py already implemented

- **Stories that depend on 3.2:**
  - Story 3.3 (Candidate Ranking) - needs parsed skills for matching
  - Story 3.4 (Semantic Search) - may use parsed requirements
  - Story 3.5 (JD Upload Frontend) - displays parsed requirements

## Testing

### Backend Tests

- **Location:** `backend/tests/modules/jobs/`
- **Framework:** `pytest` with async support (`pytest-asyncio`)
- **Test Client:** `httpx.AsyncClient` for API tests

### Test Patterns

```python
# Example: test_jd_parser.py
import pytest
from unittest.mock import AsyncMock, patch

from app.modules.jobs.jd_parser import JDParser

@pytest.fixture
def jd_parser():
    return JDParser()

@pytest.mark.asyncio
async def test_parse_jd_extracts_required_skills(jd_parser):
    jd_text = """
    We are looking for a Senior Python Developer with:
    - 3+ years of Python experience
    - Experience with FastAPI or Django
    - Knowledge of PostgreSQL
    - Nice to have: Docker, Kubernetes
    """
    
    with patch.object(jd_parser, '_call_ollama', new_callable=AsyncMock) as mock_llm:
        mock_llm.return_value = '''
        {
            "required_skills": ["Python", "FastAPI", "PostgreSQL"],
            "nice_to_have_skills": ["Docker", "Kubernetes"],
            "min_experience_years": 3,
            "job_title_normalized": "Senior Python Developer",
            "key_responsibilities": []
        }
        '''
        
        result = await jd_parser.parse_jd(jd_text)
        
        assert "python" in result.required_skills
        assert result.min_experience_years == 3
        assert "docker" in result.nice_to_have_skills


# Example: test_job_router.py - parse status
@pytest.mark.asyncio
async def test_get_parse_status_success(
    async_client: AsyncClient,
    authenticated_user_cookies: dict,
    created_jd: JobDescription  # fixture that creates a JD
):
    response = await async_client.get(
        f"/api/v1/jobs/jd/{created_jd.id}/parse-status",
        cookies=authenticated_user_cookies
    )
    assert response.status_code == 200
    data = response.json()
    assert "parse_status" in data
    assert "parsed_requirements" in data
```

**[Source: docs/architecture/testing-strategy.md]**

### Mocking Strategy

- Mock `_call_ollama` for unit tests (don't call real LLM)
- Use real database with test fixtures for integration tests
- Test status transitions: PENDING -> PROCESSING -> COMPLETED/FAILED

## Change Log

| Date | Version | Description | Author |
| :--- | :--- | :--- | :--- |
| 2025-12-18 | 1.0 | Initial draft of Story 3.2 - JD Parsing & Skill Extraction | Bob (Scrum Master) |
| 2025-12-18 | 1.1 | PO Validation: Added JDParseStatusResponse schema, parse_error field, updated line references | Sarah (PO) |

## QA Results

### Review Date: 2025-12-18

### Reviewed By: Quinn (Test Architect)

### Risk Assessment

**Review Depth: Standard** - No auto-escalation triggers detected:
- No auth/payment/security files modified
- Tests present and comprehensive (61 tests)
- Story has 10 ACs (triggers deeper review for traceability)
- Previous gate: N/A (first review)

### Code Quality Assessment

**Overall: Excellent**

The implementation demonstrates high-quality engineering practices:

1. **JDParser Class** (`jd_parser.py:79-411`):
   - Clean separation of concerns with dedicated methods for each step
   - Comprehensive docstrings with usage examples
   - Robust error handling with graceful degradation (returns empty result on failure)
   - LLM timeout handling (120s configurable)
   - JSON parsing with regex fallback for partial recovery
   - Skill normalization using existing taxonomy

2. **Service Layer** (`services.py:129-185`):
   - Clear status transition flow: PENDING -> PROCESSING -> COMPLETED/FAILED
   - Proper error message storage for debugging
   - Clean async implementation

3. **Router** (`router.py:26-209`):
   - Background task pattern using FastAPI's `BackgroundTasks`
   - Proper session management for background tasks (creates own session)
   - RESTful endpoint design with proper HTTP status codes

4. **Schemas** (`schemas.py:16-137`):
   - Well-defined Pydantic models with field validation
   - Proper enum usage for `JDParseStatus`
   - `model_config` usage for ORM compatibility

5. **Model** (`models.py:38-62`):
   - Database constraints for `parse_status` values
   - Index on `parse_status` for efficient filtering
   - VARCHAR(20) for flexibility (as recommended in Dev Notes)

### Requirements Traceability

| AC# | Acceptance Criteria | Test Coverage | Status |
|-----|---------------------|---------------|--------|
| 1 | Create JDParser class using LLM (Ollama) | `test_jd_parser.py::TestJDParserInit`, `TestJDParserParseJD` (8 tests) | COVERED |
| 2 | Extract required_skills and normalize | `test_parse_jd_extracts_required_skills`, `test_normalize_known_skills` | COVERED |
| 3 | Extract min_experience_years | `test_parse_jd_extracts_experience_years` | COVERED |
| 4 | Extract nice_to_have_skills | `test_parse_jd_extracts_required_skills` (includes nice_to_have) | COVERED |
| 5 | Normalize skills using SkillExtractor | `test_parse_jd_normalizes_skills`, `TestJDParserNormalizeSkills` (4 tests) | COVERED |
| 6 | Store in parsed_requirements JSONB | `test_parse_job_description_success`, Model check constraint | COVERED |
| 7 | Async parsing after upload | `test_create_jd_triggers_background_parsing`, `TestParseJobDescription` (4 tests) | COVERED |
| 8 | POST /api/v1/jobs/jd triggers parsing | `test_create_jd_success`, `test_create_jd_triggers_background_parsing` | COVERED |
| 9 | GET /api/v1/jobs/jd/{jd_id}/parse-status | `TestGetParseStatusEndpoint` (5 tests) | COVERED |
| 10 | Unit tests and integration tests | 61 tests total (22 parser + 8 service + 8 router) | COVERED |

**Given-When-Then Mapping (Key Scenarios):**

- **AC1-4: JD Parsing**
  - Given: A valid JD text with skills and experience requirements
  - When: `JDParser.parse_jd()` is called
  - Then: Returns `ParsedJDRequirements` with extracted data

- **AC7: Async Processing**
  - Given: A JD is created via POST /api/v1/jobs/jd
  - When: The request completes
  - Then: Background task is scheduled and user receives immediate 201 response

- **AC9: Status Endpoint**
  - Given: A JD exists with parse_status = "completed"
  - When: GET /api/v1/jobs/jd/{jd_id}/parse-status is called
  - Then: Returns parsed_requirements with extracted skills

### Test Architecture Assessment

**Test Coverage: Comprehensive (61 tests)**

| Layer | Test File | Count | Quality |
|-------|-----------|-------|---------|
| Unit | `test_jd_parser.py` | 22 | Excellent - covers all methods |
| Service | `test_job_service.py` | 17 | Good - covers CRUD + parsing |
| Router | `test_job_router.py` | 22 | Excellent - covers all endpoints |

**Strengths:**
- Proper mocking of LLM calls (no real LLM in tests)
- Error scenarios covered (timeout, exception, empty input)
- Edge cases handled (empty skills, invalid JSON, short text)
- Authentication tests included

**Test Design Quality:**
- Descriptive test names following convention
- Proper use of fixtures and async patterns
- Mock isolation prevents flaky tests

### Refactoring Performed

None required - implementation is clean and well-structured.

### Compliance Check

- Coding Standards: PASS - Follows `router.py`, `services.py`, `schemas.py`, `models.py` pattern
- Project Structure: PASS - Files in correct locations under `backend/app/modules/jobs/`
- Testing Strategy: PASS - Unit, service, and router tests present with proper mocking
- All ACs Met: PASS - All 10 acceptance criteria have corresponding tests

### Improvements Checklist

- [x] All acceptance criteria implemented and tested
- [x] Proper error handling and logging
- [x] Database migration created with proper constraints
- [x] Background task pattern implemented correctly
- [x] Skill normalization using existing taxonomy
- [ ] Consider adding rate limiting for /reparse endpoint (future)
- [ ] Add metrics/telemetry for parsing success rate (future)
- [ ] Consider caching parsed results for identical JD text (future)

### Security Review

**Status: PASS**

- Authentication required on all endpoints via `get_current_user` dependency
- Ownership check on all JD operations (`user_id` filter)
- No sensitive data exposure in error messages (truncated to 200 chars)
- LLM prompt injection risk: LOW - JD text is user's own data

### Performance Considerations

**Status: PASS**

- Background parsing prevents blocking user requests
- 120s LLM timeout is reasonable for complex parsing
- Index on `parse_status` enables efficient filtering
- Async httpx client used for non-blocking I/O

### Files Modified During Review

None - no refactoring was necessary.

### Gate Status

Gate: **PASS** -> `docs/qa/gates/3.2-jd-parsing-skill-extraction.yml`

### Recommended Status

**Ready for Done** - All acceptance criteria met, comprehensive test coverage (61 tests passing), clean implementation following project patterns.
